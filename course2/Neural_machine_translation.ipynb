{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "NMT.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4c6de4b405f14182a36af820af43af49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_da53925e7e2740f1840039aefed44f8d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_025f34d9fdb14549bddf754daab4109d",
              "IPY_MODEL_06c0c8e25a944da28e137c73f5abd511"
            ]
          }
        },
        "da53925e7e2740f1840039aefed44f8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "025f34d9fdb14549bddf754daab4109d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ac9d9b92893a4c95a5a2faae46813782",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 1805,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1101,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7695d1ffc16644d496825f43bf52f317"
          }
        },
        "06c0c8e25a944da28e137c73f5abd511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1d67172fd1bc418587e71b124b7fe644",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 61% 1100/1805 [03:26&lt;02:08,  5.50it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_70c4e1ff911142e38d704ca375dc792a"
          }
        },
        "ac9d9b92893a4c95a5a2faae46813782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7695d1ffc16644d496825f43bf52f317": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1d67172fd1bc418587e71b124b7fe644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "70c4e1ff911142e38d704ca375dc792a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bamtak/ammi-nlp/blob/master/NMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHmkZS49QExD",
        "colab_type": "text"
      },
      "source": [
        "# Neural Machine Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJOFKsHYVEfi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a3ea494-33c4-4999-eddf-f3951c86d99c"
      },
      "source": [
        "!wget \"https://nyu.box.com/shared/static/jgbcjgbhgfwmpk2y2785fzsg5vlh91iu.gz\" -O \"a.seq_to_seq.tar.gz\"\n",
        "!tar xvzf \"a.seq_to_seq.tar.gz\" -C \"./\"\n",
        "!rm \"a.seq_to_seq.tar.gz\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-17 16:31:30--  https://nyu.box.com/shared/static/jgbcjgbhgfwmpk2y2785fzsg5vlh91iu.gz\n",
            "Resolving nyu.box.com (nyu.box.com)... 107.152.29.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|107.152.29.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/jgbcjgbhgfwmpk2y2785fzsg5vlh91iu.gz [following]\n",
            "--2020-03-17 16:31:31--  https://nyu.box.com/public/static/jgbcjgbhgfwmpk2y2785fzsg5vlh91iu.gz\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/jgbcjgbhgfwmpk2y2785fzsg5vlh91iu.gz [following]\n",
            "--2020-03-17 16:31:31--  https://nyu.app.box.com/public/static/jgbcjgbhgfwmpk2y2785fzsg5vlh91iu.gz\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 107.152.29.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|107.152.29.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!-O7eAYylwQBcylHUu8vO2C2EAu6ELIfUETmgEq-4jDP18n_rGdOOKjsnlfhlYHeoawTUBPU02yJjqiYSyzwKvBx2Y3uSJKGMKvnWmUhCT-EPvMpcmdoBuA9RGTURKUsAwOI6mrvuOCp2pp2S9Dx51qhzbAp7t6wbq-fKhB05QrpB3U4jefmzU-LzK1wvZKIqveiO8vbjaOIPLIgO2eboNtvXXnlp4zFTkGNNmajdYyxxcpaLHl75Jit-MwdTyJGmSTNVFNtfCw77YZrIaUx6X_IFdmdIh--YygFFRFW8bAaRgQOLmvi8UtmgTD1luon-OwJdYWK_zVDhYVXn1KJ-OFIqRbcH3GuFK2LQM51_9d_uhdTkyOzMiWTYNIlBwBiBKBENiPWUJspcyqjhss-10f7QoBE2yhoN2wCnQoTtGWn67zqR0zDzjDG0eF042XBqLW46MRPOdKJBq1OmWToBG2olyPAo9RRbwxwSNPn0nIb57nOzVAUxhrbgUutD19TU6QOxuugGhe77C8_TE8Y0qnyV33WbaZLLVTA7TPYkMXyWguMgbAOo5IT8bW2jTxYkrLt3y0TyU0ULvstfeCanYWlECTtaJb7fLPGcZrBIN8ClqSisBetgD2AhRgPrrwwG21x0B2vBW6D-Jb7aHQhzaa6GtKATGasO-YPLWsTQvKXGSh9XS0FtVoLFpMS-7AiHdpFX9-PFqv6OUIcSjWjO4RGVGDk3v7iu0SvKR4qSF9_WHQ1FJVH4S-CsF4HplyoXjiWNAUs3-74Z6NoYK5ng-IiTYW1GBcZiXt5vgzseSLc7DRcTzmv-B2bqgwu8hSuMFLPr96G_Xd2cRdhb2nkNZMMqTI372gmrTuXKSwi9l4cKB31tuzgTr3yrHEYi5fYaSM3GoYHqPTp4EOR9-r1Jl1WGXESdMlH-2NMEQR6XHxvRVPvLDkpL4CEHRWfpPm3p5W88W7xlB2ZvUoRRjHprpyI4Erk7O8YsvYERHuzwTua2us1RTzGTZOjtLfJZZBgvTcoympkGYJPY76_rF7pPhCM8ehtZq6vjpC_5FkXeNrevw_66S8H9VB3c1OD03EtOQc0MeQzXQAP3gfXz-tQCVnmQuHe8TuOROa7Dy4JzoT1MhrNr_O2_-Xn04hlaZQqotvq3K4kTj-_ovG0UWpMsqvI4YiIIcfID4BZopCH35-gsl_IAELsBIvPSVe4rtFGyuwiy48DwO6syn5zw1jAuINzzjkxsDIUBJMKkUYSSIYFUDJPET0Nj6nC--Xa-kjdbvbkUktYim9vCvAW9tDvCoPCDuklRvaGK3Xc9dXbCca0o8EXU9bVBBpBSFNUect-FW9kImJJXHfw_lcqqRQ6pUbsVgzU7UeKAD0wUUXR5e8aUwo7pEkYQIGbucPT_ZA6gpw1y8ow./download [following]\n",
            "--2020-03-17 16:31:31--  https://public.boxcloud.com/d/1/b1!-O7eAYylwQBcylHUu8vO2C2EAu6ELIfUETmgEq-4jDP18n_rGdOOKjsnlfhlYHeoawTUBPU02yJjqiYSyzwKvBx2Y3uSJKGMKvnWmUhCT-EPvMpcmdoBuA9RGTURKUsAwOI6mrvuOCp2pp2S9Dx51qhzbAp7t6wbq-fKhB05QrpB3U4jefmzU-LzK1wvZKIqveiO8vbjaOIPLIgO2eboNtvXXnlp4zFTkGNNmajdYyxxcpaLHl75Jit-MwdTyJGmSTNVFNtfCw77YZrIaUx6X_IFdmdIh--YygFFRFW8bAaRgQOLmvi8UtmgTD1luon-OwJdYWK_zVDhYVXn1KJ-OFIqRbcH3GuFK2LQM51_9d_uhdTkyOzMiWTYNIlBwBiBKBENiPWUJspcyqjhss-10f7QoBE2yhoN2wCnQoTtGWn67zqR0zDzjDG0eF042XBqLW46MRPOdKJBq1OmWToBG2olyPAo9RRbwxwSNPn0nIb57nOzVAUxhrbgUutD19TU6QOxuugGhe77C8_TE8Y0qnyV33WbaZLLVTA7TPYkMXyWguMgbAOo5IT8bW2jTxYkrLt3y0TyU0ULvstfeCanYWlECTtaJb7fLPGcZrBIN8ClqSisBetgD2AhRgPrrwwG21x0B2vBW6D-Jb7aHQhzaa6GtKATGasO-YPLWsTQvKXGSh9XS0FtVoLFpMS-7AiHdpFX9-PFqv6OUIcSjWjO4RGVGDk3v7iu0SvKR4qSF9_WHQ1FJVH4S-CsF4HplyoXjiWNAUs3-74Z6NoYK5ng-IiTYW1GBcZiXt5vgzseSLc7DRcTzmv-B2bqgwu8hSuMFLPr96G_Xd2cRdhb2nkNZMMqTI372gmrTuXKSwi9l4cKB31tuzgTr3yrHEYi5fYaSM3GoYHqPTp4EOR9-r1Jl1WGXESdMlH-2NMEQR6XHxvRVPvLDkpL4CEHRWfpPm3p5W88W7xlB2ZvUoRRjHprpyI4Erk7O8YsvYERHuzwTua2us1RTzGTZOjtLfJZZBgvTcoympkGYJPY76_rF7pPhCM8ehtZq6vjpC_5FkXeNrevw_66S8H9VB3c1OD03EtOQc0MeQzXQAP3gfXz-tQCVnmQuHe8TuOROa7Dy4JzoT1MhrNr_O2_-Xn04hlaZQqotvq3K4kTj-_ovG0UWpMsqvI4YiIIcfID4BZopCH35-gsl_IAELsBIvPSVe4rtFGyuwiy48DwO6syn5zw1jAuINzzjkxsDIUBJMKkUYSSIYFUDJPET0Nj6nC--Xa-kjdbvbkUktYim9vCvAW9tDvCoPCDuklRvaGK3Xc9dXbCca0o8EXU9bVBBpBSFNUect-FW9kImJJXHfw_lcqqRQ6pUbsVgzU7UeKAD0wUUXR5e8aUwo7pEkYQIGbucPT_ZA6gpw1y8ow./download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 107.152.24.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|107.152.24.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 343509032 (328M) [application/octet-stream]\n",
            "Saving to: â€˜a.seq_to_seq.tar.gzâ€™\n",
            "\n",
            "a.seq_to_seq.tar.gz 100%[===================>] 327.60M  25.3MB/s    in 14s     \n",
            "\n",
            "2020-03-17 16:31:46 (23.1 MB/s) - â€˜a.seq_to_seq.tar.gzâ€™ saved [343509032/343509032]\n",
            "\n",
            "a.seq_to_seq/\n",
            "a.seq_to_seq/en2fr/\n",
            "a.seq_to_seq/en2fr/lang_obj/\n",
            "a.seq_to_seq/en2fr/lang_obj/en_lang_obj_min_count_1.p\n",
            "a.seq_to_seq/en2fr/lang_obj/fr_lang_obj_min_count_1.p\n",
            "a.seq_to_seq/en2fr/nmt_enc_bow_model_dec_rnn.pth\n",
            "a.seq_to_seq/en2fr/nmt_enc_rnn_model_dec_rnn.pth\n",
            "a.seq_to_seq/en2fr/nmt_enc_encoderattn_model_dec_rnn.pth\n",
            "a.seq_to_seq/en2fr/.ipynb_checkpoints/\n",
            "a.seq_to_seq/.ipynb_checkpoints/\n",
            "a.seq_to_seq/.ipynb_checkpoints/NMT-checkpoint.ipynb\n",
            "a.seq_to_seq/data/\n",
            "a.seq_to_seq/data/valid.fr\n",
            "a.seq_to_seq/data/valid.en\n",
            "a.seq_to_seq/data/train.fr\n",
            "a.seq_to_seq/data/train.en\n",
            "a.seq_to_seq/pyfiles/\n",
            "a.seq_to_seq/pyfiles/nnet_models_new.py\n",
            "a.seq_to_seq/pyfiles/.ipynb_checkpoints/\n",
            "a.seq_to_seq/pyfiles/.ipynb_checkpoints/nnet_models_new-checkpoint.py\n",
            "a.seq_to_seq/pyfiles/.ipynb_checkpoints/download_reqs-checkpoint.sh\n",
            "a.seq_to_seq/pyfiles/__pycache__/\n",
            "a.seq_to_seq/pyfiles/__pycache__/nmt_dataset.cpython-37.pyc\n",
            "a.seq_to_seq/pyfiles/__pycache__/global_variables.cpython-37.pyc\n",
            "a.seq_to_seq/pyfiles/__pycache__/nnet_models_new.cpython-37.pyc\n",
            "a.seq_to_seq/pyfiles/__pycache__/bleu_score.cpython-37.pyc\n",
            "a.seq_to_seq/pyfiles/__pycache__/global_variables.cpython-36.pyc\n",
            "a.seq_to_seq/pyfiles/__pycache__/nmt_dataset.cpython-36.pyc\n",
            "a.seq_to_seq/pyfiles/__pycache__/nnet_models_new.cpython-36.pyc\n",
            "a.seq_to_seq/pyfiles/__pycache__/bleu_score.cpython-36.pyc\n",
            "a.seq_to_seq/pyfiles/bleu_score.py\n",
            "a.seq_to_seq/pyfiles/nmt_dataset.py\n",
            "a.seq_to_seq/pyfiles/download_reqs.sh\n",
            "a.seq_to_seq/pyfiles/global_variables.py\n",
            "a.seq_to_seq/NMT.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgnOnKnwQExG",
        "colab_type": "text"
      },
      "source": [
        "<sup> with inputs from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html </sup>\n",
        "\n",
        "General Reference: https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW52wAfeQExI",
        "colab_type": "text"
      },
      "source": [
        "### Install Google Translate API for Comparision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz5UVYEIQExJ",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/ssut/py-googletrans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNQMIgdGQExL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "ebe45d09-c91b-496e-b4e6-c9c8571652d9"
      },
      "source": [
        "! pip install googletrans"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/f0/a22d41d3846d1f46a4f20086141e0428ccc9c6d644aacbfd30990cf46886/googletrans-2.4.0.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from googletrans) (2.21.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (1.24.3)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-2.4.0-cp36-none-any.whl size=15777 sha256=ae98435491ac142d0c18315e65dc81ba66440edbe94995fd2735125775dc1e82\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/d6/e7/a8efd5f2427d5eb258070048718fa56ee5ac57fd6f53505f95\n",
            "Successfully built googletrans\n",
            "Installing collected packages: googletrans\n",
            "Successfully installed googletrans-2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkNaBNoaQExR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from googletrans import Translator\n",
        "translator = Translator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuVIZR2PQExW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_utils = 'a.seq_to_seq/pyfiles'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2wVkAK7QExa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(path_to_utils)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE5NKlzSQExf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import global_variables\n",
        "import nmt_dataset\n",
        "import nnet_models_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7fqVi6xQExj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from functools import partial\n",
        "import time\n",
        "# from tqdm import tqdm_notebook as tqdm\n",
        "import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpX3eGrNQExn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_saved_models_dir = '.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaZ7F3JyQExr",
        "colab_type": "text"
      },
      "source": [
        "### The Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78pgkMmLQExs",
        "colab_type": "text"
      },
      "source": [
        "We will work with a English to French Dataset from https://www.manythings.org/anki/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENA00zWVQExt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main_data_path = './a.seq_to_seq/data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCtU4dsLQExx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source_name = 'en'\n",
        "target_name = 'fr'\n",
        "\n",
        "path_to_train_data = {'source':main_data_path+'train.'+source_name, \n",
        "                      'target':main_data_path+'train.'+target_name}\n",
        "path_to_val_data = {'source': main_data_path+'valid.'+source_name, \n",
        "                      'target':main_data_path+'valid.'+target_name}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY-99QN7QEx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saved_models_dir = os.path.join(base_saved_models_dir, source_name+'2'+target_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C4HetVHQEx4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5cc1497c-314e-44e6-fe30-2007634b1334"
      },
      "source": [
        "## See first 5 records\n",
        "! head -5 'a.seq_to_seq/data/train.en'"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i think we may have something that you d be interested in buying .\n",
            "they got it .\n",
            "i m glad to see you .\n",
            "he got into his car in a hurry .\n",
            "do you like mozart s music ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyXbji2QQEx7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "64b1bfea-bb5e-48e6-c5ad-8a28ea9cd122"
      },
      "source": [
        "! head -5 'a.seq_to_seq/data/train.fr'"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "je pense que nous avons peut etre quelque chose dont vous seriez interesses de faire l acquisition .\n",
            "ils l ont eue .\n",
            "je suis enchante de vous rencontrer .\n",
            "il monta en vitesse dans sa voiture .\n",
            "aimez vous la musique de mozart ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alwojp2hQEx-",
        "colab_type": "text"
      },
      "source": [
        "### Processing and making PyTorch Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKQ9EgI6QEx_",
        "colab_type": "text"
      },
      "source": [
        "We have to make it a pair - (source, target) sentence pair. For this, we have to read the file and parse it accordingly. We might have to take care of some details there, like making sure that we strip off any non-required special characters or extra space. All those boring details aside (which you can see in dataset_helper.py) what are the other things we have to do?\n",
        "\n",
        "We have to make a vocabulary and tokenize like we have been doing. Here, we are writing a Language Class, to take care of this for you. Once we have done all this and tokenized, we write a pytorch dataset object to help as handle this efficiently during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTt68wTtQEyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saved_language_model_dir = os.path.join(saved_models_dir, 'lang_obj')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gfn4B5J3QEyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_dict = {'train': nmt_dataset.LanguagePair(source_name = source_name, target_name=target_name, \n",
        "                    filepath = path_to_train_data, \n",
        "                    lang_obj_path = saved_language_model_dir,\n",
        "                     minimum_count = 1), \n",
        "\n",
        "                'val': nmt_dataset.LanguagePair(source_name = source_name, target_name=target_name, \n",
        "                    filepath = path_to_val_data, \n",
        "                    lang_obj_path = saved_language_model_dir,\n",
        "                    minimum_count = 1)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox0Tcmb1QEyH",
        "colab_type": "text"
      },
      "source": [
        "The LanguagePair object we built has a DataFrame underneath. We see the first 5 rows of the dataframe below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty8TsCe0QEyI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "44d6f157-5a31-4ed3-f00b-6617f36ef4e3"
      },
      "source": [
        "dataset_dict['train'].main_df.iloc[:5]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_data</th>\n",
              "      <th>target_data</th>\n",
              "      <th>source_tokenized</th>\n",
              "      <th>source_len</th>\n",
              "      <th>target_tokenized</th>\n",
              "      <th>target_len</th>\n",
              "      <th>source_indized</th>\n",
              "      <th>target_indized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i think we may have something that you d be in...</td>\n",
              "      <td>je pense que nous avons peut etre quelque chos...</td>\n",
              "      <td>[i, think, we, may, have, something, that, you...</td>\n",
              "      <td>15</td>\n",
              "      <td>[je, pense, que, nous, avons, peut, etre, quel...</td>\n",
              "      <td>19</td>\n",
              "      <td>[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,...</td>\n",
              "      <td>[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>they got it .</td>\n",
              "      <td>ils l ont eue .</td>\n",
              "      <td>[they, got, it, .]</td>\n",
              "      <td>5</td>\n",
              "      <td>[ils, l, ont, eue, .]</td>\n",
              "      <td>6</td>\n",
              "      <td>[18, 19, 20, 17, 3]</td>\n",
              "      <td>[22, 19, 23, 24, 21, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i m glad to see you .</td>\n",
              "      <td>je suis enchante de vous rencontrer .</td>\n",
              "      <td>[i, m, glad, to, see, you, .]</td>\n",
              "      <td>8</td>\n",
              "      <td>[je, suis, enchante, de, vous, rencontrer, .]</td>\n",
              "      <td>8</td>\n",
              "      <td>[4, 21, 22, 23, 24, 11, 17, 3]</td>\n",
              "      <td>[4, 25, 26, 17, 14, 27, 21, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>he got into his car in a hurry .</td>\n",
              "      <td>il monta en vitesse dans sa voiture .</td>\n",
              "      <td>[he, got, into, his, car, in, a, hurry, .]</td>\n",
              "      <td>10</td>\n",
              "      <td>[il, monta, en, vitesse, dans, sa, voiture, .]</td>\n",
              "      <td>9</td>\n",
              "      <td>[25, 19, 26, 27, 28, 15, 29, 30, 17, 3]</td>\n",
              "      <td>[28, 29, 30, 31, 32, 33, 34, 21, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>do you like mozart s music ?</td>\n",
              "      <td>aimez vous la musique de mozart ?</td>\n",
              "      <td>[do, you, like, mozart, s, music, ?]</td>\n",
              "      <td>8</td>\n",
              "      <td>[aimez, vous, la, musique, de, mozart, ?]</td>\n",
              "      <td>8</td>\n",
              "      <td>[31, 11, 32, 33, 34, 35, 36, 3]</td>\n",
              "      <td>[35, 14, 36, 37, 17, 38, 39, 3]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         source_data  ...                                     target_indized\n",
              "0  i think we may have something that you d be in...  ...  [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,...\n",
              "1                                      they got it .  ...                            [22, 19, 23, 24, 21, 3]\n",
              "2                              i m glad to see you .  ...                     [4, 25, 26, 17, 14, 27, 21, 3]\n",
              "3                   he got into his car in a hurry .  ...                [28, 29, 30, 31, 32, 33, 34, 21, 3]\n",
              "4                       do you like mozart s music ?  ...                    [35, 14, 36, 37, 17, 38, 39, 3]\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HddoKBSpQEyL",
        "colab_type": "text"
      },
      "source": [
        "### vocabulary sizes and sentence lengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIfNK9k5QEyM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75be60be-b7ef-4a65-e641-a56ef4825b5c"
      },
      "source": [
        "### vocabulary sizes\n",
        "print('source vocab: ', dataset_dict['train'].source_lang_obj.n_words , \n",
        "      'target vocab: ', dataset_dict['train'].target_lang_obj.n_words)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source vocab:  12392 target vocab:  20111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdHIm6c7QEyP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6c890bc-726d-4000-bac5-20347778eec3"
      },
      "source": [
        "### vocabulary sizes\n",
        "print('max len: ', dataset_dict['train'].main_df['source_len'].max(), \n",
        "      'min len: ', dataset_dict['train'].main_df['source_len'].min() )"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max len:  51 min len:  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR88MgIHQEyT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "8a20645f-b196-4b40-f8d2-fc0774a40f3e"
      },
      "source": [
        "dataset_dict['train'].main_df['source_len'].quantile([0.5, 0.75, 0.9, 0.95, 0.99, 0.999, 0.9999])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5000     8.0\n",
              "0.7500    10.0\n",
              "0.9000    12.0\n",
              "0.9500    13.0\n",
              "0.9900    17.0\n",
              "0.9990    22.0\n",
              "0.9999    32.0\n",
              "Name: source_len, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR2xNLxmQEyW",
        "colab_type": "text"
      },
      "source": [
        "51 looks like a very long sentence and at the $99.99$th percentile is 32. We probably don't want that much. How do we get rid of rest of the words or clip sentence at some MAX LEN? We can use the collate function of pytorch that we had seen earlier to do this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1cKuZEZQEyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = int(dataset_dict['train'].main_df['source_len'].quantile(0.9999))\n",
        "batchSize = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7cL_ZypQEya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloader_dict = {'train': DataLoader(dataset_dict['train'], batch_size = batchSize, \n",
        "                            collate_fn = partial(nmt_dataset.vocab_collate_func, MAX_LEN=MAX_LEN),\n",
        "                            shuffle = True, num_workers=0), \n",
        "                    'val': DataLoader(dataset_dict['val'], batch_size = batchSize, \n",
        "                            collate_fn = partial(nmt_dataset.vocab_collate_func, MAX_LEN=MAX_LEN),\n",
        "                            shuffle = True, num_workers=0) }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyheZOz2QEyd",
        "colab_type": "text"
      },
      "source": [
        "The Seq2Seq Model\n",
        "=================\n",
        "\n",
        "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
        "sequence and uses its own output as input for subsequent steps.\n",
        "\n",
        "A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n",
        "seq2seq network, or `Encoder Decoder\n",
        "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
        "consisting of usually of two RNNs called the encoder and decoder. The encoder reads\n",
        "an input sequence and outputs a single vector, and the decoder reads\n",
        "that vector to produce an output sequence. Essentially, all we need is some mechanism to read the source sentence and create an encoding and some mechanism to read the encoding and decode it to the target language. \n",
        "\n",
        "Unlike sequence prediction with a single RNN, where every input\n",
        "corresponds to an output, the seq2seq model frees us from sequence\n",
        "length and order, which makes it ideal for translation between two\n",
        "languages.\n",
        "\n",
        "Consider the sentence \"I am not the\n",
        "black cat\" â†’ \"Je ne suis pas le chat noir\". Most of the words in the input sentence have a direct\n",
        "translation in the output sentence, but are in slightly different\n",
        "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
        "construction there is also one more word in the input sentence. It would\n",
        "be difficult to produce a correct translation directly from the sequence\n",
        "of input words.\n",
        "\n",
        "With a seq2seq model the encoder creates a single vector which, in the\n",
        "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
        "vector â€” a single point in some N dimensional space of sentences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPjAqgcPQEye",
        "colab_type": "text"
      },
      "source": [
        "### Concepts:\n",
        "1. NMT as a conditional language modelling\n",
        "2. Encoder\n",
        "3. Decoding during evaluation - step by step (see code)\n",
        "4. Teaching Forcing (see code) and train step\n",
        "5. How do we evaluate the quality of translation? BLEU Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAcJ8vrBQEyf",
        "colab_type": "text"
      },
      "source": [
        "The Encoder\n",
        "-----------\n",
        "\n",
        "The encoder is anything which takes in a sentence and gives us a representation for the sentence. \n",
        "\n",
        "Usually, the encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word.\n",
        "\n",
        "However, we will first start with a BoW encoder and then move on to RNN based encoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlfeC0EYQEyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### configuration\n",
        "source_lang_obj = dataset_dict['train'].source_lang_obj\n",
        "target_lang_obj = dataset_dict['train'].target_lang_obj\n",
        "\n",
        "source_vocab = dataset_dict['train'].source_lang_obj.n_words;\n",
        "target_vocab = dataset_dict['train'].target_lang_obj.n_words;\n",
        "hidden_size = 512\n",
        "rnn_layers = 1\n",
        "lr = 0.25;\n",
        "longest_label = 32;\n",
        "gradient_clip = 0.3;\n",
        "use_cuda = True\n",
        "\n",
        "num_epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-6Y_oZAQEyi",
        "colab_type": "text"
      },
      "source": [
        "### BagOfWords Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VljKI2P4QEyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_bow = nnet_models_new.BagOfWords(input_size = source_vocab,\n",
        "                                    hidden_size = hidden_size, \n",
        "                                    nlayers=4, \n",
        "                                    reduce = \"sum\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk5HESrZQEyn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "41fb32b0-e607-4d8b-fbff-163308526d81"
      },
      "source": [
        "print(encoder_bow)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BagOfWords(\n",
            "  (embedding): Embedding(12392, 512, padding_idx=0)\n",
            "  (layers): ModuleList(\n",
            "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.1, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.1, inplace=False)\n",
            "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.1, inplace=False)\n",
            "    (9): Linear(in_features=512, out_features=512, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMUGEqHmQEyq",
        "colab_type": "text"
      },
      "source": [
        "The Decoder\n",
        "--------------------\n",
        "\n",
        "\n",
        "The decoder is another RNN that takes the encoder output vector(s) and outputs a sequence of words to create the translation.\n",
        "\n",
        "Decoder w/o Attention\n",
        "------------------------\n",
        "In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n",
        "\n",
        "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder's last hidden state)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMbVnLdAQEyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_bow = nnet_models_new.DecoderRNN(target_vocab, hidden_size, rnn_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rMdD2PpQEyt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "64d41cce-fdf9-40a3-a78f-4370caea3be7"
      },
      "source": [
        "print(decoder_bow)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecoderRNN(\n",
            "  (embedding): Embedding(20111, 512)\n",
            "  (gru): GRU(512, 512, batch_first=True)\n",
            "  (out): Linear(in_features=512, out_features=20111, bias=True)\n",
            "  (softmax): LogSoftmax()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poUV8nQvQEyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nmt_bow = nnet_models_new.seq2seq(encoder_bow, decoder_bow,\n",
        "                              lr = 1e-2, \n",
        "                              use_cuda = use_cuda, \n",
        "                              hiddensize = hidden_size, \n",
        "                              numlayers = hidden_size, \n",
        "                              target_lang=dataset_dict['train'].target_lang_obj,\n",
        "                              longest_label = longest_label,\n",
        "                              clip = gradient_clip)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDSdtmARQEy3",
        "colab_type": "text"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzRTCd9-QEy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_full_filepath(path, enc_type):\n",
        "    filename = 'nmt_enc_'+enc_type+'_dec_rnn.pth'\n",
        "    return os.path.join(path, filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dswWsMDnQEy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_models(nmt_model, path, enc_type):\n",
        "    if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "    filename = 'nmt_enc_'+enc_type+'_dec_rnn.pth'\n",
        "    torch.save(nmt_model, os.path.join(path, filename))\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIwyqQ25QEy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(dataloader, nmt, num_epochs=50, val_every=1, saved_model_path = '.', enc_type ='rnn'):\n",
        "\n",
        "    best_bleu = -1;\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        start = time.time()\n",
        "        running_loss = 0\n",
        "\n",
        "        print('Epoch: [{}/{}]'.format(epoch, num_epochs));\n",
        "        \n",
        "        for i, data in tqdm.tqdm_notebook(enumerate(dataloader['train']), total=len(dataloader['train'])):  \n",
        "            _, curr_loss = nmt.train_step(data);\n",
        "            running_loss += curr_loss\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader['train']) \n",
        "        \n",
        "        print(\"epoch {} loss = {}, time = {}\".format(epoch, epoch_loss,\n",
        "                                                        time.time() - start))\n",
        "        sys.stdout.flush()\n",
        "   \n",
        "        if epoch%val_every == 0:\n",
        "            val_bleu_score = nmt.get_bleu_score(dataloader['val']);\n",
        "            print('validation bleu: ', val_bleu_score)\n",
        "            sys.stdout.flush()\n",
        "            \n",
        "            nmt.scheduler_step(val_bleu_score);\n",
        "            \n",
        "            if val_bleu_score > best_bleu:\n",
        "                best_bleu = val_bleu_score\n",
        "                save_models(nmt, saved_model_path, enc_type);\n",
        "\n",
        "        print('='*50)\n",
        "\n",
        "    print(\"Training completed. Best BLEU is {}\".format(best_bleu))\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nneswbtlQEzB",
        "colab_type": "text"
      },
      "source": [
        "### Training Bow Encoder GRU Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYnxrxJUQEzB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "4c6de4b405f14182a36af820af43af49",
            "da53925e7e2740f1840039aefed44f8d",
            "025f34d9fdb14549bddf754daab4109d",
            "06c0c8e25a944da28e137c73f5abd511",
            "ac9d9b92893a4c95a5a2faae46813782",
            "7695d1ffc16644d496825f43bf52f317",
            "1d67172fd1bc418587e71b124b7fe644",
            "70c4e1ff911142e38d704ca375dc792a"
          ]
        },
        "outputId": "933695a8-83ab-47c3-f07f-58867d19a09c"
      },
      "source": [
        "train_again = False\n",
        "modelname = 'bow_model'\n",
        "\n",
        "device = torch.device('cuda') if use_cuda and torch.cuda.is_available() else torch.device('cpu');\n",
        "if os.path.exists(get_full_filepath(saved_models_dir, modelname)) and (not train_again):\n",
        "    nmt_bow = torch.load(get_full_filepath(saved_models_dir, modelname))\n",
        "else:\n",
        "    train_model(dataloader_dict, nmt_bow, \n",
        "                          num_epochs = num_epochs, \n",
        "                          saved_model_path = saved_models_dir, \n",
        "                          enc_type = 'bow_test')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0/10]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c6de4b405f14182a36af820af43af49",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=1805), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kswZCGgCQEzF",
        "colab_type": "text"
      },
      "source": [
        "### Check Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIMgTpsLQEzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(nmt_bow.get_bleu_score(dataloader_dict['val']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W06Va0KrQEzL",
        "colab_type": "text"
      },
      "source": [
        "## Interacting with the system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jul9ZB9OQEzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import copy\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def get_binned_bl_score(nmt_model, val_dataset):\n",
        "    \n",
        "    len_threshold = np.arange(0, 31, 5)\n",
        "    bin_bl_score = np.zeros(len(len_threshold));\n",
        "    \n",
        "    for i in tqdm.tqdm_notebook( range(1, len(len_threshold)), total = len(len_threshold)-1):\n",
        "        min_len = len_threshold[i-1]\n",
        "        max_len = len_threshold[i]\n",
        "        \n",
        "        temp_dataset = copy.deepcopy(val_dataset);\n",
        "        temp_dataset.main_df = temp_dataset.main_df[(temp_dataset.main_df['source_len'] > min_len) & (temp_dataset.main_df['source_len'] <= max_len)];\n",
        "        temp_loader = DataLoader(temp_dataset, batch_size = batchSize, \n",
        "                            collate_fn = partial(nmt_dataset.vocab_collate_func, MAX_LEN=100),\n",
        "                            shuffle = True, num_workers=0)\n",
        "        \n",
        "        bin_bl_score[i] = nmt_model.get_bleu_score(temp_loader);\n",
        "        \n",
        "    \n",
        "    len_threshold = len_threshold[1:]\n",
        "    bin_bl_score = bin_bl_score[1:]\n",
        "    \n",
        "    plt.plot(len_threshold, bin_bl_score, 'x-')\n",
        "    plt.ylim(0, np.max(bin_bl_score)+1)\n",
        "    plt.xlabel('len')\n",
        "    plt.ylabel('bl score')\n",
        "    \n",
        "    return len_threshold, bin_bl_score\n",
        "    \n",
        "        \n",
        "        \n",
        "    \n",
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions, cmap='bone', aspect='auto')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       [global_variables.EOS_TOKEN], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words.split(' ')+\n",
        "                       [global_variables.EOS_TOKEN]);\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def get_encoded_batch(sentence, lang_obj, use_cuda):\n",
        "    \"\"\" accepts only bsz = 1.\n",
        "        input: one sentence as a string\n",
        "        output: named tuple with vector and length\"\"\"\n",
        "    \n",
        "    sentence = sentence + ' ' + global_variables.EOS_TOKEN;\n",
        "    tensor = source_lang_obj.txt2vec(sentence).unsqueeze(0)\n",
        "    \n",
        "    device = torch.device('cuda') if use_cuda and torch.cuda.is_available() else torch.device('cpu');\n",
        "    \n",
        "    named_returntuple = namedtuple('namedtuple', ['text_vecs', 'text_lens', 'label_vecs', 'label_lens', 'use_packed'])\n",
        "    return_tuple = named_returntuple( tensor.to(device), \n",
        "                                     torch.from_numpy(np.array([tensor.shape[-1]])).to(device),\n",
        "                                     None,\n",
        "                                     None,\n",
        "                                     False );\n",
        "\n",
        "    return return_tuple\n",
        "\n",
        "def get_translation(nmt_model, sentence, lang_obj, use_cuda):\n",
        "    print('souce: ', sentence)\n",
        "    batch = get_encoded_batch(sentence, lang_obj, use_cuda);\n",
        "    \n",
        "    prediction, attn_scores_list = nmt_model.eval_step(batch, return_attn = True);\n",
        "    prediction = prediction[0];\n",
        "    print('prediction: ', prediction)\n",
        "    print('GT on sentence (src->tgt): ', translator.translate(sentence, \n",
        "                                                     src = source_name,\n",
        "                                                     dest = target_name).text)\n",
        "    print('GT on prediction (tgt->src): ', translator.translate(prediction, \n",
        "                                                     src = target_name,\n",
        "                                                     dest = source_name).text)\n",
        "\n",
        "    if attn_scores_list[0] is not None:\n",
        "        if attn_scores_list[0][0] is not None:\n",
        "            attn_matrix = [x[0].data.cpu().numpy() for x in attn_scores_list];\n",
        "            attn_matrix = np.stack(attn_matrix)[:,:, 0]\n",
        "            showAttention(sentence, prediction, attn_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaozR32gQEzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_translation(nmt_bow, 'how are you ?', source_lang_obj, use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7-SFnQKQEzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_translation(nmt_bow, 'are hello ? how you', source_lang_obj, use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ShOjX9hQEzV",
        "colab_type": "text"
      },
      "source": [
        "## RNN Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNyt4gjXQEzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_rnn = nnet_models_new.EncoderRNN(source_vocab, hidden_size, rnn_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWUgTNmLQEzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(encoder_rnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4io3OMoHQEze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_rnn = nnet_models_new.DecoderRNN(target_vocab, hidden_size, rnn_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhUQ6L6OQEzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(decoder_rnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnO5Qu0iQEzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nmt_rnn = nnet_models_new.seq2seq(encoder_rnn, decoder_rnn,\n",
        "                              lr = lr, \n",
        "                              use_cuda = use_cuda, \n",
        "                              hiddensize = hidden_size, \n",
        "                              numlayers = hidden_size, \n",
        "                              target_lang=dataset_dict['train'].target_lang_obj,\n",
        "                              longest_label = longest_label,\n",
        "                              clip = gradient_clip)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sLr4iTiQEzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_again = False\n",
        "modelname = 'rnn_model'\n",
        "if os.path.exists(get_full_filepath(saved_models_dir, modelname)) and (not train_again):\n",
        "    nmt_rnn = torch.load(get_full_filepath(saved_models_dir, modelname))\n",
        "else:\n",
        "    train_model(dataloader_dict, nmt_rnn, \n",
        "                      num_epochs = num_epochs, \n",
        "                      saved_model_path = saved_models_dir, \n",
        "                      enc_type = 'rnn_test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Boiiv0wCQEzo",
        "colab_type": "text"
      },
      "source": [
        "### Check Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i-NSEszQEzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(nmt_rnn.get_bleu_score(dataloader_dict['val']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr6MmkbfQEzs",
        "colab_type": "text"
      },
      "source": [
        "### Interacting with system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYOCjndEQEzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_translation(nmt_rnn, 'how are you ?', source_lang_obj, use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js_p_l7yQEzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_translation(nmt_rnn, 'are hello ? how you', source_lang_obj, use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKdomtTsQEzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_translation(nmt_rnn, 'i know that the last thing you want to do is help me .', source_lang_obj, use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y4LB3HkQEzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_len_threshold, rnn_bin_bl = get_binned_bl_score(nmt_rnn, dataset_dict['val'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGsx2WNSQEz0",
        "colab_type": "text"
      },
      "source": [
        "We work with a small training data and hence you see this drop in BLEU score for sentences of shorter length. We don't have enough data points with small sentence length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcVZNW_TQEz1",
        "colab_type": "text"
      },
      "source": [
        "## RNN Encoder + Source Side Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIF9KXb5QEz2",
        "colab_type": "text"
      },
      "source": [
        "### Concepts:\n",
        "1. Context Vector which gives additional information from source side for decoding the next token. \n",
        "2. Fit context vector into decoding framework: <br>\n",
        "    a) Initiaze with 0 <br>\n",
        "    b) input to RNN is the concatenation of the input token and context vector <br>\n",
        "    c) output token decoded from context vector calculated from the hidden of previous timestep; i.e, context vector for the next timestep is used for decoding the current output\n",
        "3. Calculating the context vector. Explain attention module: <br>\n",
        "    a) hidden transformed to encoder hidden dimension using linear layer. This is your query vector. <br> \n",
        "    b) Value and Key vectors are the encoder outputs. <br>\n",
        "    c) Dot product. Raw Score. Softmax. Linear Combination. <br>\n",
        "    d) contactenated the linear combination vector with the input hidden from decoder. Pass it through a linear layer to project it back to decoder hidden dimension followed by a `tanh`.  \n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz29UsHHQEz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_attention = True\n",
        "self_attention = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO5bkil5QEz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_encoderattn = nnet_models_new.EncoderRNN(source_vocab, hidden_size, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWqe7w4kQEz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_encoderattn = nnet_models_new.Decoder_SelfAttn(output_size=target_vocab,\n",
        "                                 hidden_size=hidden_size, \n",
        "                                 encoder_attention = encoder_attention,\n",
        "                                 self_attention = self_attention)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln_JcSY5QEz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nmt_encoderattn = nnet_models_new.seq2seq(encoder_encoderattn, decoder_encoderattn,\n",
        "                              lr = lr, \n",
        "                              use_cuda = use_cuda, \n",
        "                              hiddensize = hidden_size, \n",
        "                              numlayers = hidden_size, \n",
        "                              target_lang=dataset_dict['train'].target_lang_obj,\n",
        "                              longest_label = longest_label,\n",
        "                              clip = gradient_clip)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ldu81p1DQEz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_again = False\n",
        "modelname = 'encoderattn_model'\n",
        "if os.path.exists(get_full_filepath(saved_models_dir, modelname)) and (not train_again):\n",
        "    nmt_encoderattn = torch.load(get_full_filepath(saved_models_dir, modelname))\n",
        "else:\n",
        "    train_model(dataloader_dict, nmt_encoderattn, \n",
        "                      num_epochs = num_epochs, \n",
        "                      saved_model_path = saved_models_dir, \n",
        "                      enc_type = 'encoderattn_test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fikYv0ANQE0B",
        "colab_type": "text"
      },
      "source": [
        "### Check Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl1x6H5oQE0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(nmt_encoderattn.get_bleu_score(dataloader_dict['val']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IP28anXQE0E",
        "colab_type": "text"
      },
      "source": [
        "### BLEU vs Sentence Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEgV645aQE0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attn_len_threshold, attn_bin_bl = get_binned_bl_score(nmt_encoderattn, dataset_dict['val'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi1WRWX1QE0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(rnn_len_threshold, rnn_bin_bl, '--x', label = 'w/o attn')\n",
        "plt.plot(attn_len_threshold, attn_bin_bl, '--x', label = 'attn')\n",
        "plt.xlabel('len sentence')\n",
        "plt.ylabel('bl score')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsKZlrwjQE0J",
        "colab_type": "text"
      },
      "source": [
        "### Interacting with system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDgxhEZ0QE0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_translation(nmt_encoderattn, 'how are you ?', source_lang_obj, use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evsSbdZ4QE0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_translation(nmt_encoderattn, 'she knows better than to argue with him .', source_lang_obj, use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqRZlKbZQE0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_translation(nmt_encoderattn, 'she s five years younger than me .', source_lang_obj, use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fHH_C1vQE0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_translation(nmt_encoderattn, 'i know that the last thing you want to do is help me .', source_lang_obj, use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUdmghv3WoXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}