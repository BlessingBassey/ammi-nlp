{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Blessing_Bassey_lab1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "N4UnbCdcqHPu"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uzAMazBhyO7O"
      },
      "source": [
        "# AMMI Deep Natural Language Processing: Lab 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z2oBLu8PyV1V"
      },
      "source": [
        "## 0. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NHed26lWcGwQ"
      },
      "source": [
        "In this tutorial we will train neural networks on the bAbI tasks using ParlAI framework.  \n",
        "This tutorial can be run both in google colab or on your computer.  \n",
        "The solutions will be added during the lab [here](https://fburl.com/ammi_dnlp_lab1).  \n",
        "\n",
        "We will cover the following:\n",
        "0. Introduction\n",
        "    - Introduction to ParlAI and installation\n",
        "    - Introduction to the bAbI tasks\n",
        "1. Exploring the data:\n",
        "    - Compute some statistics (number of examples in train, valid, test, size of examples...)\n",
        "    - Look at some examples\n",
        "2. Choose the appropriate metrics\n",
        "3. Baselines\n",
        "    - Ranom baseline\n",
        "    - Majority class baseline\n",
        "    - Information retrieval baseline\n",
        "4. More elaborate models\n",
        "   - Generative model: Seq2Seq\n",
        "   - Ranking model: Memory Network\n",
        "5. To go further\n",
        "    - Additional ideas to try if you want to dig deeper\n",
        "\n",
        "### ParlAI\n",
        "[ParlAI](https://github.com/facebookresearch/ParlAI/blob/master/README.md) (pronounced “par-lay”) is a framework for dialogue AI research, implemented in Python.\n",
        "\n",
        "Its goal is to provide researchers:\n",
        "\n",
        "* a unified framework for sharing, training and testing dialogue models\n",
        "* many popular datasets available all in one place -- from open-domain chitchat to visual question answering.\n",
        "* a wide set of reference models -- from retrieval baselines to Transformers.\n",
        "* seamless integration of Amazon Mechanical Turk for data collection and human evaluation\n",
        "* integration with Facebook Messenger to connect agents with humans in a chat interface\n",
        "\n",
        "Documentation can be found [here](http://www.parl.ai/static/docs/), some of this tutorial is inspired from the ParlAI documentation so feel free to go back and forth between the notebook and the documentation.\n",
        "\n",
        "\n",
        "### Setup the notebook\n",
        "If using google colab, make sure to use TPU runtime by going to ***Runtime > Change runtime type > Hardware accelerator: TPU > Save***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-k8BZE3HJaut"
      },
      "source": [
        "### Install ParlAI\n",
        "\n",
        "Start by installing ParlAI from github. The ParlAI folder will be located in the home directory at `~/ParlAI/`.  \n",
        "*Note: In a jupyter notebook, you can run arbitrary bash commands by prefixing them with a question mark, example: `!echo \"Hello World\"`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l8V7QcnMI7Wk",
        "outputId": "15562f42-bf23-4eb7-c32e-115be963bdc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "# Remove `> /dev/null` to see the output of commands\n",
        "!git clone https://github.com/facebookresearch/ParlAI.git ~/ParlAI  > /dev/null\n",
        "!cd ~/ParlAI && git checkout 6bd0e58692b3fd3a13b5f654944525ac1b7cd8e3\n",
        "!cd ~/ParlAI && python3 setup.py develop > /dev/null"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/root/ParlAI'...\n",
            "remote: Enumerating objects: 188, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/188)\u001b[K\rremote: Counting objects:   1% (2/188)\u001b[K\rremote: Counting objects:   2% (4/188)\u001b[K\rremote: Counting objects:   3% (6/188)\u001b[K\rremote: Counting objects:   4% (8/188)\u001b[K\rremote: Counting objects:   5% (10/188)\u001b[K\rremote: Counting objects:   6% (12/188)\u001b[K\rremote: Counting objects:   7% (14/188)\u001b[K\rremote: Counting objects:   8% (16/188)\u001b[K\rremote: Counting objects:   9% (17/188)\u001b[K\rremote: Counting objects:  10% (19/188)\u001b[K\rremote: Counting objects:  11% (21/188)\u001b[K\rremote: Counting objects:  12% (23/188)\u001b[K\rremote: Counting objects:  13% (25/188)\u001b[K\rremote: Counting objects:  14% (27/188)\u001b[K\rremote: Counting objects:  15% (29/188)\u001b[K\rremote: Counting objects:  16% (31/188)\u001b[K\rremote: Counting objects:  17% (32/188)\u001b[K\rremote: Counting objects:  18% (34/188)\u001b[K\rremote: Counting objects:  19% (36/188)\u001b[K\rremote: Counting objects:  20% (38/188)\u001b[K\rremote: Counting objects:  21% (40/188)\u001b[K\rremote: Counting objects:  22% (42/188)\u001b[K\rremote: Counting objects:  23% (44/188)\u001b[K\rremote: Counting objects:  24% (46/188)\u001b[K\rremote: Counting objects:  25% (47/188)\u001b[K\rremote: Counting objects:  26% (49/188)\u001b[K\rremote: Counting objects:  27% (51/188)\u001b[K\rremote: Counting objects:  28% (53/188)\u001b[K\rremote: Counting objects:  29% (55/188)\u001b[K\rremote: Counting objects:  30% (57/188)\u001b[K\rremote: Counting objects:  31% (59/188)\u001b[K\rremote: Counting objects:  32% (61/188)\u001b[K\rremote: Counting objects:  33% (63/188)\u001b[K\rremote: Counting objects:  34% (64/188)\u001b[K\rremote: Counting objects:  35% (66/188)\u001b[K\rremote: Counting objects:  36% (68/188)\u001b[K\rremote: Counting objects:  37% (70/188)\u001b[K\rremote: Counting objects:  38% (72/188)\u001b[K\rremote: Counting objects:  39% (74/188)\u001b[K\rremote: Counting objects:  40% (76/188)\u001b[K\rremote: Counting objects:  41% (78/188)\u001b[K\rremote: Counting objects:  42% (79/188)\u001b[K\rremote: Counting objects:  43% (81/188)\u001b[K\rremote: Counting objects:  44% (83/188)\u001b[K\rremote: Counting objects:  45% (85/188)\u001b[K\rremote: Counting objects:  46% (87/188)\u001b[K\rremote: Counting objects:  47% (89/188)\u001b[K\rremote: Counting objects:  48% (91/188)\u001b[K\rremote: Counting objects:  49% (93/188)\u001b[K\rremote: Counting objects:  50% (94/188)\u001b[K\rremote: Counting objects:  51% (96/188)\u001b[K\rremote: Counting objects:  52% (98/188)\u001b[K\rremote: Counting objects:  53% (100/188)\rremote: Counting objects:  54% (102/188)\u001b[K\rremote: Counting objects:  55% (104/188)\u001b[K\rremote: Counting objects:  56% (106/188)\u001b[K\rremote: Counting objects:  57% (108/188)\u001b[K\rremote: Counting objects:  58% (110/188)\u001b[K\rremote: Counting objects:  59% (111/188)\u001b[K\rremote: Counting objects:  60% (113/188)\u001b[K\rremote: Counting objects:  61% (115/188)\u001b[K\rremote: Counting objects:  62% (117/188)\u001b[K\rremote: Counting objects:  63% (119/188)\u001b[K\rremote: Counting objects:  64% (121/188)\u001b[K\rremote: Counting objects:  65% (123/188)\u001b[K\rremote: Counting objects:  66% (125/188)\u001b[K\rremote: Counting objects:  67% (126/188)\u001b[K\rremote: Counting objects:  68% (128/188)\u001b[K\rremote: Counting objects:  69% (130/188)\u001b[K\rremote: Counting objects:  70% (132/188)\u001b[K\rremote: Counting objects:  71% (134/188)\u001b[K\rremote: Counting objects:  72% (136/188)\u001b[K\rremote: Counting objects:  73% (138/188)\u001b[K\rremote: Counting objects:  74% (140/188)\u001b[K\rremote: Counting objects:  75% (141/188)\u001b[K\rremote: Counting objects:  76% (143/188)\u001b[K\rremote: Counting objects:  77% (145/188)\u001b[K\rremote: Counting objects:  78% (147/188)\u001b[K\rremote: Counting objects:  79% (149/188)\u001b[K\rremote: Counting objects:  80% (151/188)\u001b[K\rremote: Counting objects:  81% (153/188)\u001b[K\rremote: Counting objects:  82% (155/188)\u001b[K\rremote: Counting objects:  83% (157/188)\u001b[K\rremote: Counting objects:  84% (158/188)\u001b[K\rremote: Counting objects:  85% (160/188)\u001b[K\rremote: Counting objects:  86% (162/188)\u001b[K\rremote: Counting objects:  87% (164/188)\u001b[K\rremote: Counting objects:  88% (166/188)\u001b[K\rremote: Counting objects:  89% (168/188)\u001b[K\rremote: Counting objects:  90% (170/188)\u001b[K\rremote: Counting objects:  91% (172/188)\u001b[K\rremote: Counting objects:  92% (173/188)\u001b[K\rremote: Counting objects:  93% (175/188)\u001b[K\rremote: Counting objects:  94% (177/188)\u001b[K\rremote: Counting objects:  95% (179/188)\u001b[K\rremote: Counting objects:  96% (181/188)\u001b[K\rremote: Counting objects:  97% (183/188)\u001b[K\rremote: Counting objects:  98% (185/188)\u001b[K\rremote: Counting objects:  99% (187/188)\u001b[K\rremote: Counting objects: 100% (188/188)\u001b[K\rremote: Counting objects: 100% (188/188), done.\u001b[K\n",
            "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
            "remote: Total 29959 (delta 110), reused 95 (delta 58), pack-reused 29771\u001b[K\n",
            "Receiving objects: 100% (29959/29959), 57.65 MiB | 32.69 MiB/s, done.\n",
            "Resolving deltas: 100% (21294/21294), done.\n",
            "Note: checking out '6bd0e58692b3fd3a13b5f654944525ac1b7cd8e3'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 6bd0e586 light paper link on webpage! (#1536)\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "  File \"build/bdist.linux-x86_64/egg/websocket/policyserver.py\", line 21\n",
            "    print 'Accepted connection from %s:%s' % address\n",
            "                                         ^\n",
            "SyntaxError: Missing parentheses in call to 'print'. Did you mean print('Accepted connection from %s:%s' % address)?\n",
            "\n",
            "  File \"build/bdist.linux-x86_64/egg/websocket/server.py\", line 57\n",
            "    except IOError, ex:\n",
            "                  ^\n",
            "SyntaxError: invalid syntax\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/websocket-0.2.1-py3.6.egg/websocket/policyserver.py\", line 21\n",
            "    print 'Accepted connection from %s:%s' % address\n",
            "                                         ^\n",
            "SyntaxError: Missing parentheses in call to 'print'. Did you mean print('Accepted connection from %s:%s' % address)?\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/websocket-0.2.1-py3.6.egg/websocket/server.py\", line 57\n",
            "    except IOError, ex:\n",
            "                  ^\n",
            "SyntaxError: invalid syntax\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EJDOC-zsL3wR"
      },
      "source": [
        "Most of the scripts that we will use in ParlAI are located in the `~/ParlAI/examples` directory.  \n",
        "Let's have a first glance at the scripts available, we will come back to them later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bIMjwxKrLm36",
        "outputId": "8ab218d7-8432-4c64-807c-83425efd2bc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!ls ~/ParlAI/examples/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "base_train.py\t       eval_model.py\t\t remote.py\n",
            "build_dict.py\t       extract_image_feature.py  seq2seq_train_babi.py\n",
            "build_pytorch_data.py  interactive.py\t\t train_model.py\n",
            "display_data.py        profile_train.py\n",
            "display_model.py       README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N4UnbCdcqHPu"
      },
      "source": [
        "### The bAbI tasks\n",
        "Many datasets and tasks are included in ParlAI, we will focus on the bAbI tasks.\n",
        "The bAbI tasks are 20 synthetic tasks that each test a unique aspect of text and reasoning, and hence test different capabilities of learning models from [Weston et al. ‘16](http://arxiv.org/abs/1502.05698).\n",
        "\n",
        "---\n",
        "**Question 0.**  \n",
        "Open the bAbI [paper](https://arxiv.org/pdf/1502.05698.pdf) and read the abstract  and section: *\"3 The Tasks\"* (until paragraph **Two or Three Supporting Facts**,  included).  \n",
        "- **0.a.** Explain in your own words the motivations behind these tasks (in 2-3 sentences).\n",
        "\n",
        "***ANSWER HERE***\n",
        "\n",
        "Since the main goal of Machine Learning research is producing developed system cabable of solving all tasks with no task specific engineering, the ability to perform well on all of the 20 tasks in this paper is a sign that the system is aiming at understanding language and it's capable of reasoning. In this paper, the data is produced by using simple simulation of character and object moving around and interaction in location.\n",
        "\n",
        "The task is aimed at developing the understanding of machine learing algorithm, a model should be able to able to perform well on all the 20 task provided without being fine tuned at each task, and afterwards be tested on real data set.\n",
        "\n",
        "---\n",
        "\n",
        "These tasks can be downloaded and used directly from ParlAI.  \n",
        "We will focus on tasks 1, 2 and 3, see examples below:\n",
        "\n",
        "\n",
        "**Task 1: Single Supporting Fact**  \n",
        "Mary went to the bathroom.  \n",
        "John moved to the hallway.  \n",
        "Mary travelled to the office.  \n",
        "Where is Mary?  \n",
        "**Answer: office**  \n",
        "\n",
        "\n",
        "**Task 2: Two Supporting Facts**  \n",
        "John is in the playground.  \n",
        "John picked up the football.  \n",
        "Bob went to the kitchen.  \n",
        "Where is the football?  \n",
        "**Answer: playground**\n",
        "\n",
        "\n",
        "**Task 3: Three Supporting Facts **  \n",
        "John picked up the apple.  \n",
        "John went to the office.  \n",
        "John went to the kitchen.  \n",
        "John dropped the apple.   \n",
        "Where was the apple before the kitchen?  \n",
        "**Answer: office**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCRkOVNPiP6H",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L6IZA9r0MjBd"
      },
      "source": [
        "## 1. Exploring the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1Cc-Si6SyatK"
      },
      "source": [
        "First we need to download the data, we will use the `build_dict.py` as a dummy task to download the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XAEH7xUXtxFj",
        "outputId": "cfb1e0ad-21b4-4276-8342-24014ff06e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Download the data silently\n",
        "!python ~/ParlAI/examples/build_dict.py --task babi:task1k:1 --dict-file /tmp/babi1.dict\n",
        "# Print a few examples\n",
        "!head -n 30 ~/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 1 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: train ]\n",
            "[  download_path: /root/ParlAI/downloads ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: babi:task1k:1 ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: None ]\n",
            "[  init_model: None ]\n",
            "[  model: None ]\n",
            "[  model_file: None ]\n",
            "[ PytorchData Arguments: ] \n",
            "[  batch_length_range: 5 ]\n",
            "[  batch_sort_cache_type: pop ]\n",
            "[  batch_sort_field: text ]\n",
            "[  numworkers: 4 ]\n",
            "[  pytorch_context_length: -1 ]\n",
            "[  pytorch_datapath: None ]\n",
            "[  pytorch_include_labels: True ]\n",
            "[  pytorch_preprocess: False ]\n",
            "[  pytorch_teacher_batch_sort: False ]\n",
            "[  pytorch_teacher_dataset: None ]\n",
            "[  pytorch_teacher_task: None ]\n",
            "[  shuffle: False ]\n",
            "[ Dictionary Loop Arguments: ] \n",
            "[  dict_include_test: False ]\n",
            "[  dict_include_valid: False ]\n",
            "[  dict_maxexs: -1 ]\n",
            "[  log_every_n_secs: 2 ]\n",
            "[ Dictionary Arguments: ] \n",
            "[  bpe_debug: False ]\n",
            "[  dict_endtoken: __end__ ]\n",
            "[  dict_file: /tmp/babi1.dict ]\n",
            "[  dict_initpath: None ]\n",
            "[  dict_language: english ]\n",
            "[  dict_lower: False ]\n",
            "[  dict_max_ngram_size: -1 ]\n",
            "[  dict_maxtokens: -1 ]\n",
            "[  dict_minfreq: 0 ]\n",
            "[  dict_nulltoken: __null__ ]\n",
            "[  dict_starttoken: __start__ ]\n",
            "[  dict_textfields: text,labels ]\n",
            "[  dict_tokenizer: re ]\n",
            "[  dict_unktoken: __unk__ ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[creating task(s): babi:task1k:1]\n",
            "[building data: /root/ParlAI/data/bAbI]\n",
            "[ downloading: http://parl.ai/downloads/babi/babi.tar.gz to /root/ParlAI/data/bAbI/babi.tar.gz ]\n",
            "Downloading babi.tar.gz: 100% 19.2M/19.2M [00:02<00:00, 9.30MB/s]\n",
            "unpacking babi.tar.gz\n",
            "[ running dictionary over data.. ]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-nosf/qa1_train.txt]\n",
            "Building dictionary:   0% 0.00/900 [00:00<?, ?ex/s][loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-nosf/qa1_train.txt]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-nosf/qa1_train.txt]\n",
            "Building dictionary: 100% 900/900 [00:00<00:00, 24.4kex/s]\n",
            "Dictionary: saving dictionary to /tmp/babi1.dict\n",
            "[ dictionary built with 26 tokens in 0s ]\n",
            "1 Mary moved to the bathroom.\n",
            "2 John went to the hallway.\n",
            "3 Where is Mary? \tbathroom\n",
            "4 Daniel went back to the hallway.\n",
            "5 Sandra moved to the garden.\n",
            "6 Where is Daniel? \thallway\n",
            "7 John moved to the office.\n",
            "8 Sandra journeyed to the bathroom.\n",
            "9 Where is Daniel? \thallway\n",
            "10 Mary moved to the hallway.\n",
            "11 Daniel travelled to the office.\n",
            "12 Where is Daniel? \toffice\n",
            "13 John went back to the garden.\n",
            "14 John moved to the bedroom.\n",
            "15 Where is Sandra? \tbathroom\n",
            "1 Mary went to the bedroom.\n",
            "2 John journeyed to the bathroom.\n",
            "3 Where is John? \tbathroom\n",
            "4 Sandra journeyed to the hallway.\n",
            "5 John journeyed to the garden.\n",
            "6 Where is Mary? \tbedroom\n",
            "7 John journeyed to the bathroom.\n",
            "8 Sandra journeyed to the garden.\n",
            "9 Where is John? \tbathroom\n",
            "10 Sandra went back to the bedroom.\n",
            "11 Daniel travelled to the bathroom.\n",
            "12 Where is John? \tbathroom\n",
            "13 John went to the office.\n",
            "14 Mary moved to the office.\n",
            "15 Where is Sandra? \tbedroom\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vbc5n2xpPnsb"
      },
      "source": [
        "The bAbI tasks were downloaded in `~/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-nosf/`\n",
        "\n",
        "In bAbI the data is organised as follows:\n",
        "- **Dialog turn**: A dialog turn is a single utterance / statement. Each line in the file corresponds to one dialog turn.   \n",
        "  Example: *\"John went to the office.\"*\n",
        "- **Sample (question)**: Every few dialog turns, a question can be asked that the model has to answer, this consitute a sample.  The question is followed by its ground truth answer, separated by a tab.\n",
        "  Example: *\"Where is John? `<tab>` bathroom\"*\n",
        "- **Episode**: a sequence of ordered coherent dialog turns that are related to each other form an episode. Each new episode is independant of the others. Each line starts with the dialog turn number in the current episode.\n",
        "\n",
        "\n",
        "---\n",
        "**Question 1.**\n",
        "- **1.a.** Look at the training file of task 1 (`~/ParlAI/data/bAbI/tasks_1-20_v1-2/en/qa1_train.txt`) and compute the following information:\n",
        "  - Number of episodes\n",
        "  - Number of  samples (questions)\n",
        "  - Number of dialog turns per episode\n",
        "  - How many different answers are there in the train set? How many times does each appear? (*hint: Use a python [counter](https://docs.python.org/3/library/collections.html#collections.Counter)*)\n",
        "  - How many unique words appear in the training set? How many time does each appear? (*hint: Use the Counter `most_common()` method*)\n",
        "\n",
        "*Print the answer in the following code cell*\n",
        "  \n",
        "  ---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nk7B40wOf8nU",
        "outputId": "03f41222-e409-4ad3-ad0b-f38daf808b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "# SOLUTION\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "task_1_train_path = '/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt'\n",
        "i = 0\n",
        "n_episodes = 0\n",
        "n_questions = 0\n",
        "n_total_dialog_turns = 0\n",
        "possible_answers = Counter()\n",
        "vocabulary = Counter()\n",
        "with open(task_1_train_path, 'r') as f:\n",
        "    for line in f:\n",
        "        # Each line starts with an integer giving the dialog turn in the current episode.\n",
        "        # Each episode contains dialog turns with questions (with the answer next to it separated by a tab)\n",
        "        line = line.strip('\\n')\n",
        "        dialog_turn = int(line.split(' ')[0])\n",
        "        if dialog_turn == 1:\n",
        "            n_episodes += 1\n",
        "        # Remove the dialog turn number\n",
        "        line = ' '.join(line.split(' ')[1:])\n",
        "        fields = line.split('\\t')\n",
        "        if len(fields) > 1:\n",
        "            n_questions += 1\n",
        "            possible_answers.update([fields[1]])\n",
        "        vocabulary.update(fields[0].split(' '))\n",
        "        n_total_dialog_turns += 1\n",
        "print(f'Number of episodes: {n_episodes}')\n",
        "print(f'Number of questions: {n_questions}')\n",
        "print(f'Number of dialog turns per episode: {n_total_dialog_turns/n_episodes}')\n",
        "print(f'Possible answers: {possible_answers} ({len(possible_answers)})')\n",
        "print(f'Accuracy of a random model: {1/len(possible_answers):.4f}')\n",
        "print(f'Vocabulary size: {len(vocabulary)}')\n",
        "print(f'Most common words: {vocabulary.most_common()}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of episodes: 1800\n",
            "Number of questions: 9000\n",
            "Number of dialog turns per episode: 15.0\n",
            "Possible answers: Counter({'bathroom': 1564, 'hallway': 1517, 'garden': 1508, 'bedroom': 1473, 'kitchen': 1471, 'office': 1467}) (6)\n",
            "Accuracy of a random model: 0.1667\n",
            "Vocabulary size: 24\n",
            "Most common words: [('to', 18000), ('the', 18000), ('Where', 9000), ('is', 9000), ('', 9000), ('went', 7225), ('Mary', 4535), ('Sandra', 4502), ('John', 4484), ('Daniel', 4479), ('journeyed', 3620), ('travelled', 3582), ('back', 3581), ('moved', 3573), ('bathroom.', 3070), ('hallway.', 3045), ('garden.', 2982), ('kitchen.', 2981), ('office.', 2963), ('bedroom.', 2959), ('John?', 2299), ('Mary?', 2265), ('Sandra?', 2244), ('Daniel?', 2192)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XNYNtDHiuVq8"
      },
      "source": [
        "\n",
        "- **1.b.** Use the appropriate script from the `~/ParlAI/examples/` to take a quick look at examples of the first bAbI task.  \n",
        "Does the number of episodes and examples fit what you computed before? (*hint: you can use the argument `--task babi:task1k:1` to select the first babi task*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfnPf471M5-R",
        "colab_type": "code",
        "outputId": "a217238d-9ecc-474e-d5a4-91e28ebf4233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/bin/python3: can't find '__main__' module in '/root/ParlAI/examples/'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f6HlIvzGgKv2",
        "outputId": "bbb55c88-8003-44e1-9294-a3a922f6cdac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# FILL THIS CELL \n",
        "# SOLUTION\n",
        "!python ~/ParlAI/examples/display_data.py --task babi:task10k:1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ optional arguments: ] \n",
            "[  display_ignore_fields: agent_reply ]\n",
            "[  max_display_len: 1000 ]\n",
            "[  num_examples: 10 ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 1 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: train:stream ]\n",
            "[  download_path: /root/ParlAI/downloads ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: babi:task10k:1 ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: None ]\n",
            "[  init_model: None ]\n",
            "[  model: None ]\n",
            "[  model_file: None ]\n",
            "[ PytorchData Arguments: ] \n",
            "[  batch_length_range: 5 ]\n",
            "[  batch_sort_cache_type: pop ]\n",
            "[  batch_sort_field: text ]\n",
            "[  numworkers: 4 ]\n",
            "[  pytorch_context_length: -1 ]\n",
            "[  pytorch_datapath: None ]\n",
            "[  pytorch_include_labels: True ]\n",
            "[  pytorch_preprocess: False ]\n",
            "[  pytorch_teacher_batch_sort: False ]\n",
            "[  pytorch_teacher_dataset: None ]\n",
            "[  pytorch_teacher_task: None ]\n",
            "[  shuffle: False ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[creating task(s): babi:task10k:1]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
            "[babi:task10k:1]: Mary moved to the bathroom.\n",
            "John went to the hallway.\n",
            "Where is Mary?\n",
            "[labels: bathroom]\n",
            "[label_candidates: hallway|bedroom|garden|office|bathroom|...and 1 more]\n",
            "~~\n",
            "[babi:task10k:1]: Daniel went back to the hallway.\n",
            "Sandra moved to the garden.\n",
            "Where is Daniel?\n",
            "[labels: hallway]\n",
            "[label_candidates: hallway|bedroom|garden|office|bathroom|...and 1 more]\n",
            "~~\n",
            "[babi:task10k:1]: John moved to the office.\n",
            "Sandra journeyed to the bathroom.\n",
            "Where is Daniel?\n",
            "[labels: hallway]\n",
            "[label_candidates: hallway|bedroom|garden|office|bathroom|...and 1 more]\n",
            "~~\n",
            "[babi:task10k:1]: Mary moved to the hallway.\n",
            "Daniel travelled to the office.\n",
            "Where is Daniel?\n",
            "[labels: office]\n",
            "[label_candidates: hallway|bedroom|garden|office|bathroom|...and 1 more]\n",
            "~~\n",
            "[babi:task10k:1]: John went back to the garden.\n",
            "John moved to the bedroom.\n",
            "Where is Sandra?\n",
            "[labels: bathroom]\n",
            "[label_candidates: hallway|bedroom|garden|office|bathroom|...and 1 more]\n",
            "- - - - - - - - - - - - - - - - - - - - -\n",
            "~~\n",
            "[babi:task10k:1]: Mary went to the bedroom.\n",
            "John journeyed to the bathroom.\n",
            "Where is John?\n",
            "[labels: bathroom]\n",
            "[label_candidates: hallway|bedroom|garden|office|bathroom|...and 1 more]\n",
            "~~\n",
            "[babi:task10k:1]: Sandra journeyed to the hallway.\n",
            "John journeyed to the garden.\n",
            "Where is Mary?\n",
            "[labels: bedroom]\n",
            "[label_candidates: hallway|bedroom|garden|office|bathroom|...and 1 more]\n",
            "~~\n",
            "[babi:task10k:1]: John journeyed to the bathroom.\n",
            "Sandra journeyed to the garden.\n",
            "Where is John?\n",
            "[labels: bathroom]\n",
            "[label_candidates: hallway|bedroom|garden|office|bathroom|...and 1 more]\n",
            "~~\n",
            "[babi:task10k:1]: Sandra went back to the bedroom.\n",
            "Daniel travelled to the bathroom.\n",
            "Where is John?\n",
            "[labels: bathroom]\n",
            "[label_candidates: hallway|bedroom|garden|office|bathroom|...and 1 more]\n",
            "~~\n",
            "[babi:task10k:1]: John went to the office.\n",
            "Mary moved to the office.\n",
            "Where is Sandra?\n",
            "[labels: bedroom]\n",
            "[label_candidates: hallway|bedroom|garden|office|bathroom|...and 1 more]\n",
            "- - - - - - - - - - - - - - - - - - - - -\n",
            "~~\n",
            "[ loaded 1800 episodes with a total of 9000 examples ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TovRmKOA858i"
      },
      "source": [
        "## 2. Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g9NV_vBlPSWP"
      },
      "source": [
        "The bAbI task 1 expects single word answers among a small set of possible answers.\n",
        "\n",
        "\n",
        "---\n",
        "**Question 2**  \n",
        "- **2.a.** Which metrics do you think are appropriate for evaluating a model on this task?   \n",
        "-  **2.b.**  What are their respective strengths?  \n",
        "-  **2.c.** When do they fail? (find specific examples)  \n",
        "\n",
        "\n",
        "*ANSWER HERE* \n",
        "\n",
        "(2.a) The Accuracy and the F1 Score\n",
        "\n",
        "        Strengths\n",
        "\n",
        "(2.b) __F1 Score: This type of metric is useful for retrieval based model, it is fast, scalable and reproducible.\n",
        "\n",
        "__Accuracy: This is an easy metric to implement as it summarizing a model’s predictions according to these four categories;(True positive, True negative, False positive and False negative). It works well when we have a balance data.\n",
        "\n",
        "        Weeknessess\n",
        "\n",
        "(2.c) __F1 Score: It's downside is that it do not take the true negatives into account, this score takes both false positives and false negatives into account.\n",
        "\n",
        "__Accuracy: It is bias when it comes to detecting unevenly distribution in the real world dat (which is almost always the case), and it’s one of the reasons we never use accuracy to describe the performance of machine learning models.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0_MZKCTW6fh9"
      },
      "source": [
        "## 3. Baseline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RSexIY71yiD6"
      },
      "source": [
        "We now have a clearer idea of the data distribution and the metrics that we can use.  \n",
        "The next step is to start solving the tasks with a simple baseline. This will allow us to compare more elaborate models agains this baseline.  \n",
        "Here are a few classical baselines:\n",
        "- **Random model**: The model answers randomly among the set of possible answers for each question\n",
        "-  **Majority class**: The model always answers with the most frequent answer in the training set (majority class)\n",
        "\n",
        "We are going to reimplement these own baselines.  \n",
        "Implementing a new model in ParlAI is detailed in the [tutorial](https://www.parl.ai/docs/tutorial_quick.html#add-a-simple-model) but for our simple baselines, we will only need to inherit the [Agent](https://github.com/facebookresearch/ParlAI/blob/6d246842d3f4e941dd3806f3d9fa62f607d48f59/parlai/core/agents.py#L50) class and override the `act()` method.\n",
        "\n",
        "---\n",
        "**Question 3**  \n",
        "- **3.a.** What would be the accuracy of a model that choses a random answer among the set of possible answers for each question? \n",
        "\n",
        "*ANSWER HERE*\n",
        "\n",
        "Looking at the possible answers we got above, we can see that the counter for all the possible answers are almost the same i.e they are uniformly distributed, hence the accuracy would be; 1/len(possible_answers) = 1/6 % = 0.167%\n",
        "\n",
        "---\n",
        "\n",
        "*Note: the `%%writefile` magic command in jupyter writes the content of the cell to a file at the given path.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9ahmFL0Rk6g1",
        "colab": {}
      },
      "source": [
        "!mkdir -p ~/ParlAI/parlai/agents/baseline/\n",
        "!touch ~/ParlAI/parlai/agents/baseline/random.py\n",
        "!touch ~/ParlAI/parlai/agents/baseline/majorityclass.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mZLM7DtkkJN1"
      },
      "source": [
        "- **3.b.**  Design a baseline that answers a random word in the set of possible answer (run it multiple time to observe variance in results)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zx4emBSPjmE-",
        "outputId": "9de4d2ff-1a73-4d5f-efe6-6f1a4835b304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# FILL THIS CELL\n",
        "# SOLUTION\n",
        "%%writefile ~/ParlAI/parlai/agents/baseline/random.py\n",
        "import random\n",
        "\n",
        "from parlai.core.torch_agent import Agent\n",
        "\n",
        "\n",
        "class RandomAgent(Agent):\n",
        "  \n",
        "    def act(self):\n",
        "        if 'label_candidates' not in self.observation:\n",
        "            return\n",
        "        candidates = list(self.observation['label_candidates'])\n",
        "        reply = {'text': candidates[random.randrange(len(candidates))]}\n",
        "        return reply"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /root/ParlAI/parlai/agents/baseline/random.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dMOWxgVj1Joz",
        "outputId": "bb459a18-71e5-49ed-92ff-4ac702f37cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!python ~/ParlAI/examples/eval_model.py -t babi:task10k:1 -m baseline/random | grep accuracy -A 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'exs': 1000, 'accuracy': 0.163, 'f1': 0.163, 'bleu': 1.63e-10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oVs3pOKE2-WE",
        "colab": {}
      },
      "source": [
        "!python ~/ParlAI/examples/display_model.py -t babi:task10k:1 -m baseline/random -n 10 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3UnE4oVakR4k"
      },
      "source": [
        "- **3.c.**  Design a baseline that answers the most common answer every time (majority class baseline)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hHIV2NUzGzQc",
        "outputId": "f9f5e2be-ed75-471d-9b0f-78e0c91ec183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# SOLUTION\n",
        "%%writefile ~/ParlAI/parlai/agents/baseline/majorityclass.py\n",
        "import random\n",
        "\n",
        "from parlai.core.torch_agent import Agent\n",
        "\n",
        "\n",
        "class MajorityclassAgent(Agent):\n",
        "  \n",
        "    def act(self):\n",
        "        if 'label_candidates' not in self.observation:\n",
        "            return\n",
        "        candidates = list(self.observation['label_candidates'])\n",
        "        reply = {'text': 'bathroom'}\n",
        "        return reply"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /root/ParlAI/parlai/agents/baseline/majorityclass.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U34z6bx0lJV9",
        "outputId": "13bb4416-876c-47b3-fd07-28c55655a448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!python ~/ParlAI/examples/eval_model.py -t babi:task10k:1 -m baseline/majorityclass | grep accuracy -A 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'exs': 1000, 'accuracy': 0.169, 'f1': 0.169, 'bleu': 1.69e-10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X8LHkfUS3AL0",
        "colab": {}
      },
      "source": [
        "!python ~/ParlAI/examples/display_model.py -t babi:task10k:1 -m baseline/majorityclass -n 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BOW4ZTSekWt9"
      },
      "source": [
        "---\n",
        "- **3.d.**  In which cases would the majority class baseline be better than the random baseline?\n",
        "\n",
        "*ANSWER HERE*\n",
        "\n",
        "The majority class baseline can be better than the random baseline if the counter of all possible answers differs from one another. i.e if we have an in-balaned data set. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "layL9aTK_D1a"
      },
      "source": [
        "Another slightly more advanced baseline is implemented in ParlAI: the information retrieval baseline (`ir_baseline`)\n",
        "\n",
        "---\n",
        "- **3.e.** Look at the [implementation](https://github.com/facebookresearch/ParlAI/blob/53ea58acf389bffc79c85c43bcdd848eecdcecb4/parlai/agents/ir_baseline/ir_baseline.py#L211) of the IR baseline and explain in a few lines how it works (*hint: look at the following methods `act()` `rank_candidates()`  `score_match()`*)  \n",
        "\n",
        "*ANSWER HERE*\n",
        "\n",
        "score_match(): The score_match() takes in arguments like the query, the candidates and the penalty lenght, this function canculates the score of match between the query representation of the text, this score is then used to rank candidate to get the score of each of the candidates.\n",
        "\n",
        "rank_candidates(): The rank_candidates() takes the score that was calculated above and then ranks the candidates given the representation of query, this returns the ordered list of the candidate in score ranked order.\n",
        "\n",
        "act(): This functions now uses the rank_candidates gotten above, to generate a responds (known as a reply) to the previously seen observation. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oKo6zJHwksKA"
      },
      "source": [
        "- **3.f.** Use the IR baseline and compare its with one of your baselines (random and/or majority) on bAbI tasks 1, 2 and 3.  \n",
        "    (*hint: you can use `!python ... -t babi:task1-k:{i+1}` syntax to substitute the task number in a bash command from jupyter*)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EzBAEWP7k10U",
        "outputId": "9dcfec04-3180-44f5-ad1e-479708679b84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "# SOLUTION\n",
        "for i in range(3):\n",
        "    print(f'~ Task {i+1} ~')\n",
        "    print('Random baseline:')\n",
        "    !python ~/ParlAI/examples/eval_model.py -t babi:task10k:{i+1} -m baseline/random | grep accuracy\n",
        "    if i == 0:  # Majority class was only computed for the first task (it might be something else than bathroom for task 2 and 3)\n",
        "        print('Majority class baseline:')\n",
        "        !python ~/ParlAI/examples/eval_model.py -t babi:task10k:{i+1} -m baseline/majorityclass | grep accuracy\n",
        "    print('IR baseline:')\n",
        "    !python ~/ParlAI/examples/eval_model.py -t babi:task10k:{i+1} -m ir_baseline | grep accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~ Task 1 ~\n",
            "Random baseline:\n",
            "{'exs': 1000, 'accuracy': 0.173, 'f1': 0.173, 'bleu': 1.73e-10}\n",
            "Marjority class baseline:\n",
            "{'exs': 1000, 'accuracy': 0.169, 'f1': 0.169, 'bleu': 1.69e-10}\n",
            "IR baseline:\n",
            "{'exs': 1000, 'accuracy': 0.465, 'f1': 0.465, 'hits@1': 0.465, 'hits@5': 0.961, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 4.65e-10}\n",
            "~ Task 2 ~\n",
            "Random baseline:\n",
            "{'exs': 1000, 'accuracy': 0.16, 'f1': 0.16, 'bleu': 1.6e-10}\n",
            "IR baseline:\n",
            "{'exs': 1000, 'accuracy': 0.284, 'f1': 0.284, 'hits@1': 0.284, 'hits@5': 0.9, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 2.84e-10}\n",
            "~ Task 3 ~\n",
            "Random baseline:\n",
            "{'exs': 1000, 'accuracy': 0.158, 'f1': 0.158, 'bleu': 1.58e-10}\n",
            "IR baseline:\n",
            "{'exs': 1000, 'accuracy': 0.132, 'f1': 0.132, 'hits@1': 0.132, 'hits@5': 0.836, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 1.32e-10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrt9YZTAIxA7",
        "colab_type": "text"
      },
      "source": [
        "***Observation***: From the above outputs, we can deduce that the accuracy of the IR basedline decreases with task, this is because, the task becomes more difficult. however, the Marjority class worked well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xZGFSbEwJdvx"
      },
      "source": [
        "## 4. More elaborate models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wj7onBcoyoZz"
      },
      "source": [
        "We can now continue to more elaborate models and evaluate their performance in perspective to the baselines.\n",
        "We will use the `~/ParlAI/examples/train_model.py` script. Let's first get a glance at its arguments:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gZa5Xl5Bxxxe",
        "outputId": "9269c855-8231-4020-e0de-c44235a00d6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ~/ParlAI/examples/train_model.py --help"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: train_model.py [-h] [-v] [-t TASK]\n",
            "                      [-dt {train,train:stream,train:ordered,train:ordered:stream,train:stream:ordered,train:evalmode,train:evalmode:stream,train:evalmode:ordered,train:evalmode:ordered:stream,train:evalmode:stream:ordered,valid,valid:stream,test,test:stream}]\n",
            "                      [-nt NUMTHREADS] [-bs BATCHSIZE] [-dp DATAPATH] [-m MODEL] [-mf MODEL_FILE] [-im INIT_MODEL] [-et EVALTASK]\n",
            "                      [-eps NUM_EPOCHS] [-ttim MAX_TRAIN_TIME] [-vtim VALIDATION_EVERY_N_SECS] [-stim SAVE_EVERY_N_SECS]\n",
            "                      [-sval SAVE_AFTER_VALID] [-veps VALIDATION_EVERY_N_EPOCHS] [-vp VALIDATION_PATIENCE]\n",
            "                      [-vmt VALIDATION_METRIC] [-vmm {max,min}] [-pyt PYTORCH_TEACHER_TASK] [-pytd PYTORCH_TEACHER_DATASET]\n",
            "\n",
            "Train a model\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help\n",
            "        show this help message and exit\n",
            "\n",
            "Main ParlAI Arguments:\n",
            "  -v, --show-advanced-args\n",
            "        Show hidden command line options (advanced users only) (default: False)\n",
            "  -t, --task TASK\n",
            "        ParlAI task(s), e.g. \"babi:Task1\" or \"babi,cbt\" (default: None)\n",
            "  -dt, --datatype {train,train:stream,train:ordered,train:ordered:stream,train:stream:ordered,train:evalmode,train:evalmode:stream,train:evalmode:ordered,train:evalmode:ordered:stream,train:evalmode:stream:ordered,valid,valid:stream,test,test:stream}\n",
            "        choose from: train, train:ordered, valid, test. to stream data add \":stream\" to any option (e.g., train:stream). by\n",
            "        default: train is random with replacement, valid is ordered, test is ordered. (default: train)\n",
            "  -nt, --numthreads NUMTHREADS\n",
            "        number of threads. Used for hogwild if batchsize is 1, else for number of threads in threadpool loading, (default: 1)\n",
            "  -bs, --batchsize BATCHSIZE\n",
            "        batch size for minibatch training schemes (default: 1)\n",
            "  -dp, --datapath DATAPATH\n",
            "        path to datasets, defaults to {parlai_dir}/data (default: /root/ParlAI/data)\n",
            "\n",
            "ParlAI Model Arguments:\n",
            "  -m, --model MODEL\n",
            "        the model class name. can match parlai/agents/<model> for agents in that directory, or can provide a fully specified\n",
            "        module for `from X import Y` via `-m X:Y` (e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`) (default: None)\n",
            "  -mf, --model-file MODEL_FILE\n",
            "        model file name for loading and saving models (default: None)\n",
            "  -im, --init-model INIT_MODEL\n",
            "        load model weights and dict from this file (default: None)\n",
            "\n",
            "Training Loop Arguments:\n",
            "  -et, --evaltask EVALTASK\n",
            "        task to use for valid/test (defaults to the one used for training if not set) (default: None)\n",
            "  -eps, --num-epochs NUM_EPOCHS\n",
            "  -ttim, --max-train-time MAX_TRAIN_TIME\n",
            "  -vtim, --validation-every-n-secs VALIDATION_EVERY_N_SECS\n",
            "        Validate every n seconds. Saves model to model_file (if set) whenever best val metric is found (default: -1)\n",
            "  -stim, --save-every-n-secs SAVE_EVERY_N_SECS\n",
            "        Saves the model to model_file.checkpoint after every n seconds (default -1, never). (default: -1)\n",
            "  -sval, --save-after-valid SAVE_AFTER_VALID\n",
            "        Saves the model to model_file.checkpoint after every validation (default False).\n",
            "  -veps, --validation-every-n-epochs VALIDATION_EVERY_N_EPOCHS\n",
            "        Validate every n epochs. Saves model to model_file (if set) whenever best val metric is found (default: -1)\n",
            "  -vp, --validation-patience VALIDATION_PATIENCE\n",
            "        number of iterations of validation where result does not improve before we stop training (default: 10)\n",
            "  -vmt, --validation-metric VALIDATION_METRIC\n",
            "        key into report table for selecting best validation (default: accuracy)\n",
            "  -vmm, --validation-metric-mode {max,min}\n",
            "        how to optimize validation metric (max or min) (default: None)\n",
            "\n",
            "PytorchData Arguments:\n",
            "  -pyt, --pytorch-teacher-task PYTORCH_TEACHER_TASK\n",
            "        Use the PytorchDataTeacher for multiprocessed data loading with a standard ParlAI task, e.g. \"babi:Task1k\" (default: None)\n",
            "  -pytd, --pytorch-teacher-dataset PYTORCH_TEACHER_DATASET\n",
            "        Use the PytorchDataTeacher for multiprocessed data loading with a pytorch Dataset, e.g. \"vqa_1\" or \"flickr30k\" (default:\n",
            "        None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s2apZUNj6JrS"
      },
      "source": [
        "We can train two types of models:\n",
        "- **Generative models**: The model generates an answer from its vocabulary.\n",
        "- **Ranking models**: The model is given a list of possible answers and has to choose the correct answer. This is much easier for the model since the list of possible answers is often way smaller than the size of the vocabulary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HdBwoP2K36DH"
      },
      "source": [
        "### Generative model: seq2seq with attention\n",
        "\n",
        "The generative model we are going to train is a sequence to sequence model with attention based on [Sustskever et al. 2014](https://arxiv.org/abs/1409.3215) and [Bahdanau et al. 2014](https://arxiv.org/abs/1409.0473).\n",
        "      \n",
        "- **4.a.** Briefly explain how attention works in sequence to sequence neural networks.\n",
        "- **4.b.** Do you think attention is useful for the babi tasks? How would you verify it experimentally?\n",
        "\n",
        "*ANSWER HERE*\n",
        "\n",
        "4.a. The first paper explains that prior to the time the paper was written, the neural network cannot be used to map seq2seq, they now came up with a multilayered Long Short-Term Memory to map the input sequence to a vector of a fixed dimensionality; and then another deep LSTM to decode the target sequence from the vector, however, this method suffers a bit from long sentences, especially those that are longer than the sentences in the training corpus. hence, In order to address this issue, the second paper introduce an extension to the encoder–decoder model which learns to align and translate jointly. This is how it works, Each time the proposed model generates a word in a translation, using the attention mecahnism, it searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.\n",
        "\n",
        "4.b. Yes, I think the attention wouldn't be so useful for the bAbi tasks, this is because, after running the seq2seq model above, we observed that it was as good as the simple ft-Idf baseline, which had an accuracy of 46.5% and that of the seq2seq neural network was 52.6%, we can see that the margin is not quit large, hence, the attention mechanism wouldn't be so useful for the bAbi tasks. \n",
        "\n",
        "\n",
        "---\n",
        "- **4.c.** Train a seq2seq on bAbI task 1 (10k) and compare its results to the baselines.\n",
        "   (*hint: for faster training use the following arguments `--batchsize 32 --numthreads 1 --num-epochs 5 --hiddensize 64 --embeddingsize 64 --numlayers 1 --decoder shared`)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VIpwQj4NpY5s",
        "outputId": "7c2d7811-0181-4a47-d8b4-73b040938bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# FILL THIS CELL\n",
        "# SOLUTION\n",
        "!python ~/ParlAI/examples/train_model.py --task babi:task10k:1 --model seq2seq  --model-file /tmp/babi_s2s --batchsize 32 --numthreads 1 --num-epochs 5 --hiddensize 64 --embeddingsize 64 --numlayers 1 --decoder shared"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 32 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: train ]\n",
            "[  download_path: /root/ParlAI/downloads ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: babi:task10k:1 ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
            "[  init_model: None ]\n",
            "[  model: seq2seq ]\n",
            "[  model_file: /tmp/babi_s2s ]\n",
            "[ Training Loop Arguments: ] \n",
            "[  dict_build_first: True ]\n",
            "[  display_examples: False ]\n",
            "[  eval_batchsize: None ]\n",
            "[  evaltask: None ]\n",
            "[  load_from_checkpoint: False ]\n",
            "[  max_train_time: -1 ]\n",
            "[  num_epochs: 5.0 ]\n",
            "[  save_after_valid: False ]\n",
            "[  save_every_n_secs: -1 ]\n",
            "[  validation_cutoff: 1.0 ]\n",
            "[  validation_every_n_epochs: -1 ]\n",
            "[  validation_every_n_secs: -1 ]\n",
            "[  validation_max_exs: -1 ]\n",
            "[  validation_metric: accuracy ]\n",
            "[  validation_metric_mode: None ]\n",
            "[  validation_patience: 10 ]\n",
            "[  validation_share_agent: False ]\n",
            "[ Tensorboard Arguments: ] \n",
            "[  tensorboard_comment:  ]\n",
            "[  tensorboard_log: False ]\n",
            "[  tensorboard_metrics: None ]\n",
            "[  tensorboard_tag: None ]\n",
            "[ PytorchData Arguments: ] \n",
            "[  batch_length_range: 5 ]\n",
            "[  batch_sort_cache_type: pop ]\n",
            "[  batch_sort_field: text ]\n",
            "[  numworkers: 4 ]\n",
            "[  pytorch_context_length: -1 ]\n",
            "[  pytorch_datapath: None ]\n",
            "[  pytorch_include_labels: True ]\n",
            "[  pytorch_preprocess: False ]\n",
            "[  pytorch_teacher_batch_sort: False ]\n",
            "[  pytorch_teacher_dataset: None ]\n",
            "[  pytorch_teacher_task: None ]\n",
            "[  shuffle: False ]\n",
            "[ Dictionary Loop Arguments: ] \n",
            "[  dict_include_test: False ]\n",
            "[  dict_include_valid: False ]\n",
            "[  dict_maxexs: -1 ]\n",
            "[  log_every_n_secs: 2 ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[ Seq2Seq Arguments: ] \n",
            "[  attention: none ]\n",
            "[  attention_length: 48 ]\n",
            "[  attention_time: post ]\n",
            "[  bidirectional: False ]\n",
            "[  decoder: shared ]\n",
            "[  dropout: 0.1 ]\n",
            "[  embeddingsize: 64 ]\n",
            "[  hiddensize: 64 ]\n",
            "[  input_dropout: 0.0 ]\n",
            "[  lookuptable: unique ]\n",
            "[  numlayers: 1 ]\n",
            "[  numsoftmax: 1 ]\n",
            "[  rnn_class: lstm ]\n",
            "[ Torch Generator Agent: ] \n",
            "[  beam_block_ngram: 0 ]\n",
            "[  beam_dot_log: False ]\n",
            "[  beam_min_length: 1 ]\n",
            "[  beam_min_n_best: 3 ]\n",
            "[  beam_size: 1 ]\n",
            "[  skip_generation: False ]\n",
            "[ TorchAgent Arguments: ] \n",
            "[  add_p1_after_newln: False ]\n",
            "[  betas: (0.9, 0.999) ]\n",
            "[  delimiter: \n",
            " ]\n",
            "[  embedding_projection: random ]\n",
            "[  embedding_type: random ]\n",
            "[  gpu: -1 ]\n",
            "[  gradient_clip: 0.1 ]\n",
            "[  history_size: -1 ]\n",
            "[  label_truncate: None ]\n",
            "[  learningrate: 1 ]\n",
            "[  lr_scheduler: reduceonplateau ]\n",
            "[  lr_scheduler_decay: 0.5 ]\n",
            "[  lr_scheduler_patience: 3 ]\n",
            "[  momentum: 0 ]\n",
            "[  nesterov: True ]\n",
            "[  no_cuda: False ]\n",
            "[  nus: (0.7,) ]\n",
            "[  optimizer: sgd ]\n",
            "[  person_tokens: False ]\n",
            "[  rank_candidates: False ]\n",
            "[  split_lines: False ]\n",
            "[  text_truncate: None ]\n",
            "[  truncate: -1 ]\n",
            "[  update_freq: -1 ]\n",
            "[  use_reply: label ]\n",
            "[  warmup_rate: 0.0001 ]\n",
            "[  warmup_updates: -1 ]\n",
            "[ Dictionary Arguments: ] \n",
            "[  bpe_debug: False ]\n",
            "[  dict_endtoken: __end__ ]\n",
            "[  dict_file: None ]\n",
            "[  dict_initpath: None ]\n",
            "[  dict_language: english ]\n",
            "[  dict_lower: False ]\n",
            "[  dict_max_ngram_size: -1 ]\n",
            "[  dict_maxtokens: -1 ]\n",
            "[  dict_minfreq: 0 ]\n",
            "[  dict_nulltoken: __null__ ]\n",
            "[  dict_starttoken: __start__ ]\n",
            "[  dict_textfields: text,labels ]\n",
            "[  dict_tokenizer: re ]\n",
            "[  dict_unktoken: __unk__ ]\n",
            "[ building dictionary first... ]\n",
            "[creating task(s): babi:task10k:1]\n",
            "[ running dictionary over data.. ]\n",
            "Building dictionary:   0% 0.00/9.00k [00:00<?, ?ex/s][loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
            "Building dictionary:  86% 7.76k/9.00k [00:00<00:00, 25.9kex/s][loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
            "Building dictionary: 100% 9.00k/9.00k [00:00<00:00, 25.9kex/s]\n",
            "Dictionary: saving dictionary to /tmp/babi_s2s.dict\n",
            "[ dictionary built with 26 tokens in 0s ]\n",
            "[ no model with opt yet at: /tmp/babi_s2s(.opt) ]\n",
            "Dictionary: loading dictionary from /tmp/babi_s2s.dict\n",
            "[ num words =  26 ]\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "[creating task(s): babi:task10k:1]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
            "[ training... ]\n",
            "[ time:2.0s total_exs:576 epochs:0.06 time_left:156.0s ] {'exs': 576, 'lr': 1, 'num_updates': 18, 'loss': 36.45, 'token_acc': 0.4531, 'nll_loss': 2.025, 'ppl': 7.575}\n",
            "[ time:4.0s total_exs:1120 epochs:0.12 time_left:159.0s ] {'exs': 544, 'lr': 1, 'num_updates': 35, 'loss': 19.05, 'token_acc': 0.5919, 'nll_loss': 1.121, 'ppl': 3.067}\n",
            "[ time:6.0s total_exs:1728 epochs:0.19 time_left:152.0s ] {'exs': 608, 'lr': 1, 'num_updates': 54, 'loss': 19.97, 'token_acc': 0.5872, 'nll_loss': 1.051, 'ppl': 2.861}\n",
            "[ time:8.0s total_exs:2304 epochs:0.26 time_left:150.0s ] {'exs': 576, 'lr': 1, 'num_updates': 72, 'loss': 18.54, 'token_acc': 0.5938, 'nll_loss': 1.03, 'ppl': 2.801}\n",
            "[ time:10.0s total_exs:2880 epochs:0.32 time_left:150.0s ] {'exs': 576, 'lr': 1, 'num_updates': 90, 'loss': 18.15, 'token_acc': 0.5903, 'nll_loss': 1.009, 'ppl': 2.742}\n",
            "[ time:12.0s total_exs:3520 epochs:0.39 time_left:147.0s ] {'exs': 640, 'lr': 1, 'num_updates': 110, 'loss': 19.55, 'token_acc': 0.6086, 'nll_loss': 0.9777, 'ppl': 2.658}\n",
            "[ time:14.0s total_exs:4160 epochs:0.46 time_left:144.0s ] {'exs': 640, 'lr': 1, 'num_updates': 130, 'loss': 19.56, 'token_acc': 0.5938, 'nll_loss': 0.9778, 'ppl': 2.659}\n",
            "[ time:16.0s total_exs:4800 epochs:0.53 time_left:141.0s ] {'exs': 640, 'lr': 1, 'num_updates': 150, 'loss': 19.89, 'token_acc': 0.5883, 'nll_loss': 0.9945, 'ppl': 2.703}\n",
            "[ time:18.0s total_exs:5440 epochs:0.6 time_left:138.0s ] {'exs': 640, 'lr': 1, 'num_updates': 170, 'loss': 19.68, 'token_acc': 0.5867, 'nll_loss': 0.9838, 'ppl': 2.675}\n",
            "[ time:21.0s total_exs:6080 epochs:0.68 time_left:136.0s ] {'exs': 640, 'lr': 1, 'num_updates': 190, 'loss': 19.36, 'token_acc': 0.5945, 'nll_loss': 0.9682, 'ppl': 2.633}\n",
            "[ time:23.0s total_exs:6720 epochs:0.75 time_left:133.0s ] {'exs': 640, 'lr': 1, 'num_updates': 210, 'loss': 19.64, 'token_acc': 0.5844, 'nll_loss': 0.9818, 'ppl': 2.669}\n",
            "[ time:25.0s total_exs:7328 epochs:0.81 time_left:131.0s ] {'exs': 608, 'lr': 1, 'num_updates': 229, 'loss': 18.23, 'token_acc': 0.5946, 'nll_loss': 0.9597, 'ppl': 2.611}\n",
            "[ time:27.0s total_exs:7936 epochs:0.88 time_left:129.0s ] {'exs': 608, 'lr': 1, 'num_updates': 248, 'loss': 18.38, 'token_acc': 0.6044, 'nll_loss': 0.9673, 'ppl': 2.631}\n",
            "[ time:29.0s total_exs:8480 epochs:0.94 time_left:128.0s ] {'exs': 544, 'lr': 1, 'num_updates': 265, 'loss': 16.14, 'token_acc': 0.5956, 'nll_loss': 0.9494, 'ppl': 2.584}\n",
            "[ time:31.0s total_exs:9120 epochs:1.01 time_left:125.0s ] {'exs': 640, 'lr': 1, 'num_updates': 285, 'loss': 18.95, 'token_acc': 0.6023, 'nll_loss': 0.9475, 'ppl': 2.579}\n",
            "[ time:33.0s total_exs:9760 epochs:1.08 time_left:123.0s ] {'exs': 640, 'lr': 1, 'num_updates': 305, 'loss': 18.23, 'token_acc': 0.632, 'nll_loss': 0.9117, 'ppl': 2.489}\n",
            "[ time:35.0s total_exs:10368 epochs:1.15 time_left:121.0s ] {'exs': 608, 'lr': 1, 'num_updates': 324, 'loss': 17.49, 'token_acc': 0.6275, 'nll_loss': 0.9204, 'ppl': 2.51}\n",
            "[ time:38.0s total_exs:10976 epochs:1.22 time_left:119.0s ] {'exs': 608, 'lr': 1, 'num_updates': 343, 'loss': 17.66, 'token_acc': 0.6266, 'nll_loss': 0.9294, 'ppl': 2.533}\n",
            "[ time:40.0s total_exs:11520 epochs:1.28 time_left:117.0s ] {'exs': 544, 'lr': 1, 'num_updates': 360, 'loss': 14.98, 'token_acc': 0.6425, 'nll_loss': 0.8811, 'ppl': 2.413}\n",
            "[ time:42.0s total_exs:12160 epochs:1.35 time_left:115.0s ] {'exs': 640, 'lr': 1, 'num_updates': 380, 'loss': 17.71, 'token_acc': 0.6359, 'nll_loss': 0.8853, 'ppl': 2.424}\n",
            "[ time:44.0s total_exs:12800 epochs:1.42 time_left:112.0s ] {'exs': 640, 'lr': 1, 'num_updates': 400, 'loss': 16.99, 'token_acc': 0.6578, 'nll_loss': 0.8493, 'ppl': 2.338}\n",
            "[ time:46.0s total_exs:13440 epochs:1.49 time_left:110.0s ] {'exs': 640, 'lr': 1, 'num_updates': 420, 'loss': 16.84, 'token_acc': 0.6781, 'nll_loss': 0.842, 'ppl': 2.321}\n",
            "[ time:48.0s total_exs:14080 epochs:1.56 time_left:108.0s ] {'exs': 640, 'lr': 1, 'num_updates': 440, 'loss': 17.21, 'token_acc': 0.6641, 'nll_loss': 0.8605, 'ppl': 2.364}\n",
            "[ time:50.0s total_exs:14688 epochs:1.63 time_left:106.0s ] {'exs': 608, 'lr': 1, 'num_updates': 459, 'loss': 15.93, 'token_acc': 0.6826, 'nll_loss': 0.8382, 'ppl': 2.312}\n",
            "[ time:52.0s total_exs:15232 epochs:1.69 time_left:104.0s ] {'exs': 544, 'lr': 1, 'num_updates': 476, 'loss': 14.17, 'token_acc': 0.6838, 'nll_loss': 0.8337, 'ppl': 2.302}\n",
            "[ time:55.0s total_exs:15808 epochs:1.76 time_left:102.0s ] {'exs': 576, 'lr': 1, 'num_updates': 494, 'loss': 14.39, 'token_acc': 0.7014, 'nll_loss': 0.7992, 'ppl': 2.224}\n",
            "[ time:57.0s total_exs:16320 epochs:1.81 time_left:101.0s ] {'exs': 512, 'lr': 1, 'num_updates': 510, 'loss': 13.24, 'token_acc': 0.71, 'nll_loss': 0.8276, 'ppl': 2.288}\n",
            "[ time:59.0s total_exs:16928 epochs:1.88 time_left:99.0s ] {'exs': 608, 'lr': 1, 'num_updates': 529, 'loss': 15.31, 'token_acc': 0.7023, 'nll_loss': 0.8055, 'ppl': 2.238}\n",
            "[ time:61.0s total_exs:17504 epochs:1.94 time_left:97.0s ] {'exs': 576, 'lr': 1, 'num_updates': 547, 'loss': 13.93, 'token_acc': 0.7057, 'nll_loss': 0.7741, 'ppl': 2.169}\n",
            "[ time:63.0s total_exs:18080 epochs:2.01 time_left:95.0s ] {'exs': 576, 'lr': 1, 'num_updates': 565, 'loss': 14.37, 'token_acc': 0.7066, 'nll_loss': 0.7984, 'ppl': 2.222}\n",
            "[ time:65.0s total_exs:18720 epochs:2.08 time_left:92.0s ] {'exs': 640, 'lr': 1, 'num_updates': 585, 'loss': 15.76, 'token_acc': 0.7125, 'nll_loss': 0.7878, 'ppl': 2.199}\n",
            "[ time:67.0s total_exs:19360 epochs:2.15 time_left:90.0s ] {'exs': 640, 'lr': 1, 'num_updates': 605, 'loss': 15.59, 'token_acc': 0.7266, 'nll_loss': 0.7797, 'ppl': 2.181}\n",
            "[ time:69.0s total_exs:19968 epochs:2.22 time_left:88.0s ] {'exs': 608, 'lr': 1, 'num_updates': 624, 'loss': 14.74, 'token_acc': 0.7048, 'nll_loss': 0.776, 'ppl': 2.173}\n",
            "[ time:71.0s total_exs:20512 epochs:2.28 time_left:86.0s ] {'exs': 544, 'lr': 1, 'num_updates': 641, 'loss': 12.75, 'token_acc': 0.7252, 'nll_loss': 0.75, 'ppl': 2.117}\n",
            "[ time:73.0s total_exs:21120 epochs:2.35 time_left:84.0s ] {'exs': 608, 'lr': 1, 'num_updates': 660, 'loss': 14.98, 'token_acc': 0.7113, 'nll_loss': 0.7885, 'ppl': 2.2}\n",
            "[ time:76.0s total_exs:21760 epochs:2.42 time_left:82.0s ] {'exs': 640, 'lr': 1, 'num_updates': 680, 'loss': 15.11, 'token_acc': 0.7188, 'nll_loss': 0.7554, 'ppl': 2.128}\n",
            "[ time:78.0s total_exs:22400 epochs:2.49 time_left:80.0s ] {'exs': 640, 'lr': 1, 'num_updates': 700, 'loss': 15.31, 'token_acc': 0.7063, 'nll_loss': 0.7655, 'ppl': 2.15}\n",
            "[ time:80.0s total_exs:23008 epochs:2.56 time_left:77.0s ] {'exs': 608, 'lr': 1, 'num_updates': 719, 'loss': 13.95, 'token_acc': 0.7229, 'nll_loss': 0.7342, 'ppl': 2.084}\n",
            "[ time:82.0s total_exs:23616 epochs:2.62 time_left:75.0s ] {'exs': 608, 'lr': 1, 'num_updates': 738, 'loss': 14.48, 'token_acc': 0.7196, 'nll_loss': 0.7622, 'ppl': 2.143}\n",
            "[ time:84.0s total_exs:24160 epochs:2.68 time_left:73.0s ] {'exs': 544, 'lr': 1, 'num_updates': 755, 'loss': 12.44, 'token_acc': 0.7316, 'nll_loss': 0.732, 'ppl': 2.079}\n",
            "[ time:86.0s total_exs:24768 epochs:2.75 time_left:71.0s ] {'exs': 608, 'lr': 1, 'num_updates': 774, 'loss': 14.0, 'token_acc': 0.7393, 'nll_loss': 0.7369, 'ppl': 2.089}\n",
            "[ time:88.0s total_exs:25408 epochs:2.82 time_left:69.0s ] {'exs': 640, 'lr': 1, 'num_updates': 794, 'loss': 14.05, 'token_acc': 0.7422, 'nll_loss': 0.7024, 'ppl': 2.019}\n",
            "[ time:90.0s total_exs:26016 epochs:2.89 time_left:67.0s ] {'exs': 608, 'lr': 1, 'num_updates': 813, 'loss': 13.77, 'token_acc': 0.727, 'nll_loss': 0.7246, 'ppl': 2.064}\n",
            "[ time:92.0s total_exs:26592 epochs:2.95 time_left:65.0s ] {'exs': 576, 'lr': 1, 'num_updates': 831, 'loss': 12.51, 'token_acc': 0.7595, 'nll_loss': 0.6952, 'ppl': 2.004}\n",
            "[ time:94.0s total_exs:27200 epochs:3.02 time_left:63.0s ] {'exs': 608, 'lr': 1, 'num_updates': 850, 'loss': 13.37, 'token_acc': 0.7344, 'nll_loss': 0.7039, 'ppl': 2.022}\n",
            "[ time:97.0s total_exs:27840 epochs:3.09 time_left:60.0s ] {'exs': 640, 'lr': 1, 'num_updates': 870, 'loss': 14.6, 'token_acc': 0.7367, 'nll_loss': 0.7301, 'ppl': 2.075}\n",
            "[ time:99.0s total_exs:28480 epochs:3.16 time_left:58.0s ] {'exs': 640, 'lr': 1, 'num_updates': 890, 'loss': 14.24, 'token_acc': 0.7523, 'nll_loss': 0.712, 'ppl': 2.038}\n",
            "[ time:101.0s total_exs:29120 epochs:3.24 time_left:56.0s ] {'exs': 640, 'lr': 1, 'num_updates': 910, 'loss': 13.37, 'token_acc': 0.7609, 'nll_loss': 0.6684, 'ppl': 1.951}\n",
            "[ time:103.0s total_exs:29728 epochs:3.3 time_left:54.0s ] {'exs': 608, 'lr': 1, 'num_updates': 929, 'loss': 12.57, 'token_acc': 0.7533, 'nll_loss': 0.6617, 'ppl': 1.938}\n",
            "[ time:105.0s total_exs:30336 epochs:3.37 time_left:52.0s ] {'exs': 608, 'lr': 1, 'num_updates': 948, 'loss': 13.23, 'token_acc': 0.7508, 'nll_loss': 0.6964, 'ppl': 2.007}\n",
            "[ time:107.0s total_exs:30880 epochs:3.43 time_left:50.0s ] {'exs': 544, 'lr': 1, 'num_updates': 965, 'loss': 11.6, 'token_acc': 0.7509, 'nll_loss': 0.6823, 'ppl': 1.978}\n",
            "[ time:109.0s total_exs:31488 epochs:3.5 time_left:48.0s ] {'exs': 608, 'lr': 1, 'num_updates': 984, 'loss': 12.8, 'token_acc': 0.7599, 'nll_loss': 0.6738, 'ppl': 1.962}\n",
            "[ time:111.0s total_exs:32096 epochs:3.57 time_left:45.0s ] {'exs': 608, 'lr': 1, 'num_updates': 1003, 'loss': 12.29, 'token_acc': 0.7549, 'nll_loss': 0.6466, 'ppl': 1.909}\n",
            "[ time:113.0s total_exs:32640 epochs:3.63 time_left:44.0s ] {'exs': 544, 'lr': 1, 'num_updates': 1020, 'loss': 11.58, 'token_acc': 0.7555, 'nll_loss': 0.681, 'ppl': 1.976}\n",
            "[ time:115.0s total_exs:33280 epochs:3.7 time_left:41.0s ] {'exs': 640, 'lr': 1, 'num_updates': 1040, 'loss': 13.51, 'token_acc': 0.7688, 'nll_loss': 0.6754, 'ppl': 1.965}\n",
            "[ time:118.0s total_exs:33920 epochs:3.77 time_left:39.0s ] {'exs': 640, 'lr': 1, 'num_updates': 1060, 'loss': 13.39, 'token_acc': 0.7555, 'nll_loss': 0.6697, 'ppl': 1.954}\n",
            "[ time:120.0s total_exs:34560 epochs:3.84 time_left:37.0s ] {'exs': 640, 'lr': 1, 'num_updates': 1080, 'loss': 13.25, 'token_acc': 0.7516, 'nll_loss': 0.6625, 'ppl': 1.94}\n",
            "[ time:122.0s total_exs:35200 epochs:3.91 time_left:35.0s ] {'exs': 640, 'lr': 1, 'num_updates': 1100, 'loss': 13.45, 'token_acc': 0.7484, 'nll_loss': 0.6726, 'ppl': 1.959}\n",
            "[ time:124.0s total_exs:35808 epochs:3.98 time_left:32.0s ] {'exs': 608, 'lr': 1, 'num_updates': 1119, 'loss': 12.76, 'token_acc': 0.7393, 'nll_loss': 0.6718, 'ppl': 1.958}\n",
            "[ time:126.0s total_exs:36320 epochs:4.04 time_left:31.0s ] {'exs': 512, 'lr': 1, 'num_updates': 1135, 'loss': 10.53, 'token_acc': 0.75, 'nll_loss': 0.6581, 'ppl': 1.931}\n",
            "[ time:128.0s total_exs:36928 epochs:4.1 time_left:29.0s ] {'exs': 608, 'lr': 1, 'num_updates': 1154, 'loss': 12.2, 'token_acc': 0.7475, 'nll_loss': 0.6421, 'ppl': 1.9}\n",
            "[ time:130.0s total_exs:37536 epochs:4.17 time_left:26.0s ] {'exs': 608, 'lr': 1, 'num_updates': 1173, 'loss': 12.07, 'token_acc': 0.75, 'nll_loss': 0.6353, 'ppl': 1.888}\n",
            "[ time:132.0s total_exs:38080 epochs:4.23 time_left:25.0s ] {'exs': 544, 'lr': 1, 'num_updates': 1190, 'loss': 10.55, 'token_acc': 0.7564, 'nll_loss': 0.6204, 'ppl': 1.86}\n",
            "[ time:134.0s total_exs:38720 epochs:4.3 time_left:22.0s ] {'exs': 640, 'lr': 1, 'num_updates': 1210, 'loss': 13.03, 'token_acc': 0.7422, 'nll_loss': 0.6517, 'ppl': 1.919}\n",
            "[ time:136.0s total_exs:39328 epochs:4.37 time_left:20.0s ] {'exs': 608, 'lr': 1, 'num_updates': 1229, 'loss': 12.65, 'token_acc': 0.741, 'nll_loss': 0.6658, 'ppl': 1.946}\n",
            "[ time:138.0s total_exs:39936 epochs:4.44 time_left:18.0s ] {'exs': 608, 'lr': 1, 'num_updates': 1248, 'loss': 12.2, 'token_acc': 0.7385, 'nll_loss': 0.6423, 'ppl': 1.901}\n",
            "[ time:140.0s total_exs:40480 epochs:4.5 time_left:16.0s ] {'exs': 544, 'lr': 1, 'num_updates': 1265, 'loss': 11.17, 'token_acc': 0.7417, 'nll_loss': 0.6573, 'ppl': 1.93}\n",
            "[ time:142.0s total_exs:41088 epochs:4.57 time_left:14.0s ] {'exs': 608, 'lr': 1, 'num_updates': 1284, 'loss': 11.91, 'token_acc': 0.7558, 'nll_loss': 0.6271, 'ppl': 1.872}\n",
            "[ time:144.0s total_exs:41664 epochs:4.63 time_left:12.0s ] {'exs': 576, 'lr': 1, 'num_updates': 1302, 'loss': 10.99, 'token_acc': 0.7648, 'nll_loss': 0.6108, 'ppl': 1.842}\n",
            "[ time:147.0s total_exs:42240 epochs:4.69 time_left:10.0s ] {'exs': 576, 'lr': 1, 'num_updates': 1320, 'loss': 12.19, 'token_acc': 0.737, 'nll_loss': 0.677, 'ppl': 1.968}\n",
            "[ time:149.0s total_exs:42880 epochs:4.76 time_left:8.0s ] {'exs': 640, 'lr': 1, 'num_updates': 1340, 'loss': 13.15, 'token_acc': 0.7477, 'nll_loss': 0.6576, 'ppl': 1.93}\n",
            "[ time:151.0s total_exs:43488 epochs:4.83 time_left:6.0s ] {'exs': 608, 'lr': 1, 'num_updates': 1359, 'loss': 11.95, 'token_acc': 0.7451, 'nll_loss': 0.6291, 'ppl': 1.876}\n",
            "[ time:153.0s total_exs:44064 epochs:4.9 time_left:4.0s ] {'exs': 576, 'lr': 1, 'num_updates': 1377, 'loss': 10.94, 'token_acc': 0.7752, 'nll_loss': 0.6077, 'ppl': 1.836}\n",
            "[ time:155.0s total_exs:44640 epochs:4.96 time_left:2.0s ] {'exs': 576, 'lr': 1, 'num_updates': 1395, 'loss': 11.43, 'token_acc': 0.7474, 'nll_loss': 0.6351, 'ppl': 1.887}\n",
            "[ time:156.0s total_exs:45024 epochs:5.0 time_left:0s ] {'exs': 384, 'lr': 1, 'num_updates': 1407, 'loss': 6.825, 'token_acc': 0.7617, 'nll_loss': 0.5688, 'ppl': 1.766}\n",
            "[ num_epochs completed:5.0 time elapsed:156.54190230369568s ]\n",
            "[creating task(s): babi:task10k:1]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
            "[ running eval: valid ]\n",
            "valid:{'exs': 1000, 'accuracy': 0.557, 'f1': 0.557, 'bleu': 5.57e-10, 'lr': 1, 'num_updates': 1407, 'loss': 19.94, 'token_acc': 0.7785, 'nll_loss': 0.5565, 'ppl': 1.745}\n",
            "[creating task(s): babi:task10k:1]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_test.txt]\n",
            "[ running eval: test ]\n",
            "test:{'exs': 1000, 'accuracy': 0.526, 'f1': 0.526, 'bleu': 5.26e-10, 'lr': 1, 'num_updates': 1407, 'loss': 20.66, 'token_acc': 0.763, 'nll_loss': 0.5965, 'ppl': 1.816}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OvxjgtEHTXFb",
        "outputId": "8fbdd966-84d0-4a2c-b462-1d667f2afbfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ~/ParlAI/examples/display_model.py --task babi:task10k:1 --model seq2seq --model-file /tmp/babi_s2s"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ optional arguments: ] \n",
            "[  display_ignore_fields:  ]\n",
            "[  num_examples: 10 ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 1 ]\n",
            "[  datapath: /root/ParlAI/data ]\n",
            "[  datatype: valid ]\n",
            "[  download_path: /root/ParlAI/downloads ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: babi:task10k:1 ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
            "[  init_model: None ]\n",
            "[  model: seq2seq ]\n",
            "[  model_file: /tmp/babi_s2s ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "[ Seq2Seq Arguments: ] \n",
            "[  attention: none ]\n",
            "[  attention_length: 48 ]\n",
            "[  attention_time: post ]\n",
            "[  bidirectional: False ]\n",
            "[  decoder: same ]\n",
            "[  dropout: 0.1 ]\n",
            "[  embeddingsize: 128 ]\n",
            "[  hiddensize: 128 ]\n",
            "[  input_dropout: 0.0 ]\n",
            "[  lookuptable: unique ]\n",
            "[  numlayers: 2 ]\n",
            "[  numsoftmax: 1 ]\n",
            "[  rnn_class: lstm ]\n",
            "[ Torch Generator Agent: ] \n",
            "[  beam_block_ngram: 0 ]\n",
            "[  beam_dot_log: False ]\n",
            "[  beam_min_length: 1 ]\n",
            "[  beam_min_n_best: 3 ]\n",
            "[  beam_size: 1 ]\n",
            "[  skip_generation: False ]\n",
            "[ TorchAgent Arguments: ] \n",
            "[  add_p1_after_newln: False ]\n",
            "[  betas: (0.9, 0.999) ]\n",
            "[  delimiter: \n",
            " ]\n",
            "[  embedding_projection: random ]\n",
            "[  embedding_type: random ]\n",
            "[  gpu: -1 ]\n",
            "[  gradient_clip: 0.1 ]\n",
            "[  history_size: -1 ]\n",
            "[  label_truncate: None ]\n",
            "[  learningrate: 1 ]\n",
            "[  lr_scheduler: reduceonplateau ]\n",
            "[  lr_scheduler_decay: 0.5 ]\n",
            "[  lr_scheduler_patience: 3 ]\n",
            "[  momentum: 0 ]\n",
            "[  nesterov: True ]\n",
            "[  no_cuda: False ]\n",
            "[  nus: (0.7,) ]\n",
            "[  optimizer: sgd ]\n",
            "[  person_tokens: False ]\n",
            "[  rank_candidates: False ]\n",
            "[  split_lines: False ]\n",
            "[  text_truncate: None ]\n",
            "[  truncate: -1 ]\n",
            "[  update_freq: -1 ]\n",
            "[  use_reply: label ]\n",
            "[  warmup_rate: 0.0001 ]\n",
            "[  warmup_updates: -1 ]\n",
            "[ Dictionary Arguments: ] \n",
            "[  bpe_debug: False ]\n",
            "[  dict_endtoken: __end__ ]\n",
            "[  dict_file: None ]\n",
            "[  dict_initpath: None ]\n",
            "[  dict_language: english ]\n",
            "[  dict_lower: False ]\n",
            "[  dict_max_ngram_size: -1 ]\n",
            "[  dict_maxtokens: -1 ]\n",
            "[  dict_minfreq: 0 ]\n",
            "[  dict_nulltoken: __null__ ]\n",
            "[  dict_starttoken: __start__ ]\n",
            "[  dict_textfields: text,labels ]\n",
            "[  dict_tokenizer: re ]\n",
            "[  dict_unktoken: __unk__ ]\n",
            "Dictionary: loading dictionary from /tmp/babi_s2s.dict\n",
            "[ num words =  26 ]\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "[ Loading existing model params from /tmp/babi_s2s ]\n",
            "/root/ParlAI/parlai/core/torch_agent.py:727: UserWarning: LR scheduler is different from saved. Starting fresh!\n",
            "  warn_once(\"LR scheduler is different from saved. Starting fresh!\")\n",
            "[creating task(s): babi:task10k:1]\n",
            "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
            "[eval_labels_choice]: bathroom\n",
            "[babi:task10k:1]: Sandra travelled to the office.\n",
            "Sandra went to the bathroom.\n",
            "Where is Sandra?\n",
            "[label_candidates: kitchen|bathroom|office|bedroom|hallway|...and 1 more]\n",
            "[eval_labels: bathroom]\n",
            "   [Seq2Seq]: bathroom\n",
            "~~\n",
            "[eval_labels_choice]: bathroom\n",
            "[babi:task10k:1]: Sandra travelled to the office.\n",
            "Sandra went to the bathroom.\n",
            "Where is Sandra?\n",
            "bathroom\n",
            "Mary went to the bedroom.\n",
            "Daniel moved to the hallway.\n",
            "Where is Sandra?\n",
            "[label_candidates: kitchen|bathroom|office|bedroom|hallway|...and 1 more]\n",
            "[eval_labels: bathroom]\n",
            "   [Seq2Seq]: hallway\n",
            "~~\n",
            "[eval_labels_choice]: bathroom\n",
            "[babi:task10k:1]: Sandra travelled to the office.\n",
            "Sandra went to the bathroom.\n",
            "Where is Sandra?\n",
            "bathroom\n",
            "Mary went to the bedroom.\n",
            "Daniel moved to the hallway.\n",
            "Where is Sandra?\n",
            "bathroom\n",
            "John went to the garden.\n",
            "John travelled to the office.\n",
            "Where is Sandra?\n",
            "[label_candidates: kitchen|bathroom|office|bedroom|hallway|...and 1 more]\n",
            "[eval_labels: bathroom]\n",
            "   [Seq2Seq]: office\n",
            "~~\n",
            "[eval_labels_choice]: office\n",
            "[babi:task10k:1]: Sandra travelled to the office.\n",
            "Sandra went to the bathroom.\n",
            "Where is Sandra?\n",
            "bathroom\n",
            "Mary went to the bedroom.\n",
            "Daniel moved to the hallway.\n",
            "Where is Sandra?\n",
            "bathroom\n",
            "John went to the garden.\n",
            "John travelled to the office.\n",
            "Where is Sandra?\n",
            "bathroom\n",
            "Daniel journeyed to the bedroom.\n",
            "Daniel travelled to the hallway.\n",
            "Where is John?\n",
            "[label_candidates: kitchen|bathroom|office|bedroom|hallway|...and 1 more]\n",
            "[eval_labels: office]\n",
            "   [Seq2Seq]: bedroom\n",
            "~~\n",
            "[eval_labels_choice]: hallway\n",
            "[babi:task10k:1]: Sandra travelled to the office.\n",
            "Sandra went to the bathroom.\n",
            "Where is Sandra?\n",
            "bathroom\n",
            "Mary went to the bedroom.\n",
            "Daniel moved to the hallway.\n",
            "Where is Sandra?\n",
            "bathroom\n",
            "John went to the garden.\n",
            "John travelled to the office.\n",
            "Where is Sandra?\n",
            "bathroom\n",
            "Daniel journeyed to the bedroom.\n",
            "Daniel travelled to the hallway.\n",
            "Where is John?\n",
            "office\n",
            "John went to the bedroom.\n",
            "John travelled to the office.\n",
            "Where is Daniel?\n",
            "[label_candidates: kitchen|bathroom|office|bedroom|hallway|...and 1 more]\n",
            "[eval_labels: hallway]\n",
            "   [Seq2Seq]: office\n",
            "- - - - - - - - - - - - - - - - - - - - -\n",
            "~~\n",
            "[eval_labels_choice]: garden\n",
            "[babi:task10k:1]: Sandra went back to the bathroom.\n",
            "Mary moved to the garden.\n",
            "Where is Mary?\n",
            "[label_candidates: kitchen|bathroom|office|bedroom|hallway|...and 1 more]\n",
            "[eval_labels: garden]\n",
            "   [Seq2Seq]: bathroom\n",
            "~~\n",
            "[eval_labels_choice]: office\n",
            "[babi:task10k:1]: Sandra went back to the bathroom.\n",
            "Mary moved to the garden.\n",
            "Where is Mary?\n",
            "garden\n",
            "Mary went back to the hallway.\n",
            "Sandra went to the office.\n",
            "Where is Sandra?\n",
            "[label_candidates: kitchen|bathroom|office|bedroom|hallway|...and 1 more]\n",
            "[eval_labels: office]\n",
            "   [Seq2Seq]: office\n",
            "~~\n",
            "[eval_labels_choice]: office\n",
            "[babi:task10k:1]: Sandra went back to the bathroom.\n",
            "Mary moved to the garden.\n",
            "Where is Mary?\n",
            "garden\n",
            "Mary went back to the hallway.\n",
            "Sandra went to the office.\n",
            "Where is Sandra?\n",
            "office\n",
            "John went back to the hallway.\n",
            "John travelled to the office.\n",
            "Where is Sandra?\n",
            "[label_candidates: kitchen|bathroom|office|bedroom|hallway|...and 1 more]\n",
            "[eval_labels: office]\n",
            "   [Seq2Seq]: office\n",
            "~~\n",
            "[eval_labels_choice]: office\n",
            "[babi:task10k:1]: Sandra went back to the bathroom.\n",
            "Mary moved to the garden.\n",
            "Where is Mary?\n",
            "garden\n",
            "Mary went back to the hallway.\n",
            "Sandra went to the office.\n",
            "Where is Sandra?\n",
            "office\n",
            "John went back to the hallway.\n",
            "John travelled to the office.\n",
            "Where is Sandra?\n",
            "office\n",
            "Sandra journeyed to the hallway.\n",
            "Daniel moved to the office.\n",
            "Where is John?\n",
            "[label_candidates: kitchen|bathroom|office|bedroom|hallway|...and 1 more]\n",
            "[eval_labels: office]\n",
            "   [Seq2Seq]: office\n",
            "~~\n",
            "[eval_labels_choice]: office\n",
            "[babi:task10k:1]: Sandra went back to the bathroom.\n",
            "Mary moved to the garden.\n",
            "Where is Mary?\n",
            "garden\n",
            "Mary went back to the hallway.\n",
            "Sandra went to the office.\n",
            "Where is Sandra?\n",
            "office\n",
            "John went back to the hallway.\n",
            "John travelled to the office.\n",
            "Where is Sandra?\n",
            "office\n",
            "Sandra journeyed to the hallway.\n",
            "Daniel moved to the office.\n",
            "Where is John?\n",
            "office\n",
            "Mary went to the office.\n",
            "Sandra went to the office.\n",
            "Where is John?\n",
            "[label_candidates: kitchen|bathroom|office|bedroom|hallway|...and 1 more]\n",
            "[eval_labels: office]\n",
            "   [Seq2Seq]: office\n",
            "- - - - - - - - - - - - - - - - - - - - -\n",
            "~~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DaX_GSktxrz1"
      },
      "source": [
        "### Ranking model: memory network\n",
        "\n",
        "We saw in the class that Memory Networks ([Sukhbaatar et al. 15'](https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf)) rely on an explicit memory \"database\". this is especially adapted to tasks where a few useful memories are \"hidden\" among distractor memories.  \n",
        "These type of networks worktherefore  especially well for the bAbI tasks by turning the previous dialog turns as memories and the question as the query.  \n",
        "Here is an illustration of how a memory network work:\n",
        "\n",
        "![Memory Network schema](https://raw.githubusercontent.com/louismartin/ammi-2019-bordes-DeepNLP/master/lab1/memory_network.png)\n",
        "\n",
        "\n",
        "---\n",
        "**Question 4**  \n",
        "- **4.d.** Explain how hops work in a memory network (either with words or formulas using the notations of the above figure)\n",
        "- **4.e.** How can a memory network be used to rank multiple candidates?  \n",
        "  (*hint: you can look at the [implementation](https://github.com/facebookresearch/ParlAI/blob/6bd0e58692b3fd3a13b5f654944525ac1b7cd8e3/parlai/agents/memnn/modules.py#L22) of the memory network in ParlAI and especially the `_score()` method. Recall how the IR baseline worked.*)\n",
        "  \n",
        "*ANSWER HERE*\n",
        "  \n",
        "• 4.d. This is how it works, firstly, one defines a memory, which is a possibly very large ar-ray of slots which can encode both long-term and short-term context. At test time one is given a query(e.g. the question in QA tasks), which is used to iteratively address and read from the memory looking for relevant information to answer the question. At each step, the collected information from the memory is cumulatively added to the original query to build context for the next round. At the last iteration, the final retrieved context and the most recent query are combined as features to predict a response from a list of candidates. (these iterations are referred to as “hops”) Memory Network hop outputs attention-weighted sum of memory embeddings. The memory network operates on memory with multiple hops by vary  the  number  of  hops  and  memory  size. From the reasearch, It was shown that We  increasing the number ofhops helps improve results.\n",
        "\n",
        "\n",
        "• 4.e. The score() takes in the output from the hop, as well as the candidates, and then calculates the score of match using the dot product, this score is then passed into a softmax to get the probability, this is then used to rank multiple candidates\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YHICORdji1Cx"
      },
      "source": [
        "\n",
        "- **4.f.** Using the ParlAI implementation, train a memory network on bAbI tasks 1, 2 and 3 (10k) and compare its results with the baselines.  \n",
        "   (*hint: use a 1 thread, a batch size of 32 and 5 epochs*)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iKeyD3UYsaFs",
        "outputId": "1b0ca8db-47a1-46e4-fbbe-8ea450147187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        }
      },
      "source": [
        "# SOLUTION\n",
        "for i in range(3):\n",
        "    print(f'~ Task {i+1} ~')\n",
        "    !python ~/ParlAI/examples/train_model.py -t babi:task10k:{i+1} -m memnn -mf /tmp/babi{i+1}_memnn -bs 32 -eps 5 | grep \"'accuracy':\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~ Task 1 ~\n",
            "Building dictionary: 100% 9.00k/9.00k [00:00<00:00, 25.3kex/s]\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing train mode with provided inline set of candidates ]\n",
            "  ''.format(mode)\n",
            "/root/ParlAI/parlai/core/torch_ranker_agent.py:214: UserWarning: Some training metrics are omitted for speed. Set the flag `--train-predict` to calculate train metrics.\n",
            "  \"Some training metrics are omitted for speed. Set the flag \"\n",
            "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing eval mode with provided inline set of candidates ]\n",
            "  ''.format(mode)\n",
            "valid:{'exs': 1000, 'accuracy': 0.989, 'f1': 0.989, 'hits@1': 0.989, 'hits@5': 1.0, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 9.89e-10, 'lr': 1, 'num_updates': 1407, 'examples': 1000, 'loss': 40.14, 'mean_loss': 0.04014, 'mean_rank': 1.013}\n",
            "test:{'exs': 1000, 'accuracy': 0.991, 'f1': 0.991, 'hits@1': 0.991, 'hits@5': 1.0, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 9.91e-10, 'lr': 1, 'num_updates': 1407, 'examples': 1000, 'loss': 33.36, 'mean_loss': 0.03336, 'mean_rank': 1.009}\n",
            "~ Task 2 ~\n",
            "Building dictionary: 100% 9.00k/9.00k [00:00<00:00, 18.5kex/s]\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing train mode with provided inline set of candidates ]\n",
            "  ''.format(mode)\n",
            "/root/ParlAI/parlai/core/torch_ranker_agent.py:214: UserWarning: Some training metrics are omitted for speed. Set the flag `--train-predict` to calculate train metrics.\n",
            "  \"Some training metrics are omitted for speed. Set the flag \"\n",
            "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing eval mode with provided inline set of candidates ]\n",
            "  ''.format(mode)\n",
            "valid:{'exs': 1000, 'accuracy': 0.206, 'f1': 0.206, 'hits@1': 0.206, 'hits@5': 0.857, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 2.06e-10, 'lr': 1, 'num_updates': 1407, 'examples': 1000, 'loss': 2535.0, 'mean_loss': 2.535, 'mean_rank': 3.304}\n",
            "test:{'exs': 1000, 'accuracy': 0.178, 'f1': 0.178, 'hits@1': 0.178, 'hits@5': 0.858, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 1.78e-10, 'lr': 1, 'num_updates': 1407, 'examples': 1000, 'loss': 2538.0, 'mean_loss': 2.538, 'mean_rank': 3.42}\n",
            "~ Task 3 ~\n",
            "Building dictionary: 100% 9.00k/9.00k [00:01<00:00, 8.27kex/s]\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing train mode with provided inline set of candidates ]\n",
            "  ''.format(mode)\n",
            "/root/ParlAI/parlai/core/torch_ranker_agent.py:214: UserWarning: Some training metrics are omitted for speed. Set the flag `--train-predict` to calculate train metrics.\n",
            "  \"Some training metrics are omitted for speed. Set the flag \"\n",
            "/root/ParlAI/parlai/core/torch_ranker_agent.py:376: UserWarning: [ Executing eval mode with provided inline set of candidates ]\n",
            "  ''.format(mode)\n",
            "valid:{'exs': 1000, 'accuracy': 0.335, 'f1': 0.335, 'hits@1': 0.335, 'hits@5': 0.984, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 3.35e-10, 'lr': 1, 'num_updates': 1407, 'examples': 1000, 'loss': 1786.0, 'mean_loss': 1.786, 'mean_rank': 2.572}\n",
            "test:{'exs': 1000, 'accuracy': 0.376, 'f1': 0.376, 'hits@1': 0.376, 'hits@5': 0.976, 'hits@10': 1.0, 'hits@100': 1.0, 'bleu': 3.76e-10, 'lr': 1, 'num_updates': 1407, 'examples': 1000, 'loss': 1769.0, 'mean_loss': 1.769, 'mean_rank': 2.544}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv1iLocZdw7N",
        "colab_type": "text"
      },
      "source": [
        "***Answer Here***\n",
        "\n",
        "Training a memory network on bAbI tasks 1, 2 and 3 (10k) and comparing its results with the baselines. we notice the following:\n",
        "\n",
        "The accuracy of the random baseline and that of the on the memory network 3 taskes are as follows:\n",
        "\n",
        "                   Random Baseline                \n",
        "Task 1:                 0.173                           \n",
        "Task 2:                 0.16                           \n",
        "Task 3:                 0.158                              \n",
        "\n",
        "                   Memory Network                \n",
        "Task 1:                 0.991                          \n",
        "Task 2:                 0.178                         \n",
        "Task 3:                 0.376\n",
        "\n",
        "As seen above, it is obvious that the memory network out perform more that the random baseline, however, we can still work on the accuracies of task 2 and 3 in order to make it better. Maybe increasing the hop might help. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DHH0Wy34o3_W"
      },
      "source": [
        "## 5. To go further"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zaNGupqLssUB"
      },
      "source": [
        "If you want to go further you can try to do the following:\n",
        "\n",
        "- Retrieve and plot the attention of the memory network for the different hops along the memories.\n",
        "- For the seq2seq model, can you plot the training loss? The validation loss? Both on the same plot?\n",
        "- Can you show an example of overfitting?\n",
        "- Adapt the seq2seq model for ranking using the [torch ranker tutorial](http://www.parl.ai/static/docs/tutorial_torch_ranker_agent.html)\n",
        "- Try multitasking babi and squad, does it improve the performance? (this will require more GPU power than what is available in google colab)\n",
        "- You can play around with other models and other tasks\n",
        "- Try interfacing ParlAI with [messenger](https://parl.ai/docs/tutorial_chat_service.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M7RKRV9cI2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}